{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT AND DRIVE SETUP"
      ],
      "metadata": {
        "id": "p2zZhKDpUUjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyPu_qxmMPE8",
        "outputId": "0619deaf-5a4d-44ac-a272-630941f25943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install numpy pandas scipy scikit-learn matplotlib seaborn joblib h5py xgboost lightgbm tensorflow"
      ],
      "metadata": {
        "id": "hT-9m5CuQ0kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import seaborn\n",
        "import joblib\n",
        "import h5py\n",
        "import xgboost\n",
        "import tensorflow as tf\n",
        "import lightgbm\n",
        "import os,pprint\n",
        "from scipy.interpolate import PchipInterpolator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import traceback"
      ],
      "metadata": {
        "id": "FLz07P5pQMEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFdqQMs7RI4E",
        "outputId": "15873e8d-e9df-4d3c-e05e-f66695b3ee12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AE_FILE = \"/content/drive/My Drive/Aurora_Prediction/AE dataset/AE dataset.txt\"\n",
        "MAG_INPUT_FOLDER = \"/content/drive/My Drive/Aurora_Prediction/MAG dataset/\"\n",
        "\n",
        "BASE_DIR = os.path.dirname(os.path.dirname(AE_FILE))\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\") if os.path.exists(os.path.join(BASE_DIR, \"data\")) else BASE_DIR\n",
        "\n",
        "RAW_COMBINED = os.path.join(BASE_DIR, \"MAG_L2_raw_combined.csv\")\n",
        "FEATURES_CSV = os.path.join(BASE_DIR, \"MAG_L2_features.csv\")\n",
        "SCALED_CSV = os.path.join(BASE_DIR, \"MAG_L2_features_scaled.csv\")\n",
        "FINAL_CSV = os.path.join(BASE_DIR, \"MAG_L2_final_features.csv\")\n",
        "MODEL_DATASET_CSV = os.path.join(BASE_DIR, \"MAG_L2_model_dataset.csv\")\n",
        "\n",
        "SCALER_STAGE1 = os.path.join(BASE_DIR, \"mag_scaler_stage1.joblib\")\n",
        "SCALER_FINAL = os.path.join(BASE_DIR, \"mag_scaler_final.joblib\")\n",
        "\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "for sub in ['classification','regression','data_analytics']:\n",
        "    os.makedirs(os.path.join(MODELS_DIR, sub), exist_ok=True)\n",
        "    os.makedirs(os.path.join(PLOTS_DIR, sub), exist_ok=True)\n",
        "\n",
        "# --- Runtime flags ---\n",
        "FAST_MODE = True            # Set False for full training runs\n",
        "RUN_EXTRACT = True\n",
        "RUN_FEATURES = True\n",
        "RUN_ADVANCED_FEATURES = True\n",
        "RUN_REGRESSION = True\n",
        "RUN_MERGE_AE = True\n",
        "RUN_CLASSIFICATION = True\n",
        "RUN_ANALYTICS = True\n",
        "\n",
        "# --- Print confirmation ---\n",
        "print(\"Paths and flags set — verify below:\")\n",
        "pprint.pprint({\n",
        "    \"AE_FILE\": AE_FILE,\n",
        "    \"MAG_INPUT_FOLDER\": MAG_INPUT_FOLDER,\n",
        "    \"BASE_DIR\": BASE_DIR,\n",
        "    \"RAW_COMBINED\": RAW_COMBINED,\n",
        "    \"FEATURES_CSV\": FEATURES_CSV,\n",
        "    \"SCALED_CSV\": SCALED_CSV,\n",
        "    \"FINAL_CSV\": FINAL_CSV,\n",
        "    \"MODEL_DATASET_CSV\": MODEL_DATASET_CSV,\n",
        "    \"SCALER_STAGE1\": SCALER_STAGE1,\n",
        "    \"SCALER_FINAL\": SCALER_FINAL,\n",
        "    \"MODELS_DIR\": MODELS_DIR,\n",
        "    \"PLOTS_DIR\": PLOTS_DIR,\n",
        "    \"FAST_MODE\": FAST_MODE,\n",
        "    \"RUN_EXTRACT\": RUN_EXTRACT,\n",
        "    \"RUN_FEATURES\": RUN_FEATURES,\n",
        "    \"RUN_REGRESSION\": RUN_REGRESSION,\n",
        "    \"RUN_MERGE_AE\": RUN_MERGE_AE\n",
        "})\n",
        "print(\"\\nChecking that the specified input paths exist:\")\n",
        "print(\"AE_FILE exists:\", os.path.exists(AE_FILE))\n",
        "print(\"MAG_INPUT_FOLDER exists:\", os.path.exists(MAG_INPUT_FOLDER))\n",
        "print(\"List MAG_INPUT_FOLDER (first 10 entries):\")\n",
        "if os.path.exists(MAG_INPUT_FOLDER):\n",
        "    print(sorted(os.listdir(MAG_INPUT_FOLDER))[:10])\n",
        "else:\n",
        "    print(\"MAG input folder not found. Please check the path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy_YuO20RGWZ",
        "outputId": "b5867555-f87e-4a1d-d4c7-a912ea7e2d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paths and flags set — verify below:\n",
            "{'AE_FILE': '/content/drive/My Drive/Aurora_Prediction/AE dataset/AE '\n",
            "            'dataset.txt',\n",
            " 'BASE_DIR': '/content/drive/My Drive/Aurora_Prediction',\n",
            " 'FAST_MODE': True,\n",
            " 'FEATURES_CSV': '/content/drive/My '\n",
            "                 'Drive/Aurora_Prediction/MAG_L2_features.csv',\n",
            " 'FINAL_CSV': '/content/drive/My '\n",
            "              'Drive/Aurora_Prediction/MAG_L2_final_features.csv',\n",
            " 'MAG_INPUT_FOLDER': '/content/drive/My Drive/Aurora_Prediction/MAG dataset/',\n",
            " 'MODELS_DIR': '/content/drive/My Drive/Aurora_Prediction/models',\n",
            " 'MODEL_DATASET_CSV': '/content/drive/My '\n",
            "                      'Drive/Aurora_Prediction/MAG_L2_model_dataset.csv',\n",
            " 'PLOTS_DIR': '/content/drive/My Drive/Aurora_Prediction/plots',\n",
            " 'RAW_COMBINED': '/content/drive/My '\n",
            "                 'Drive/Aurora_Prediction/MAG_L2_raw_combined.csv',\n",
            " 'RUN_EXTRACT': True,\n",
            " 'RUN_FEATURES': True,\n",
            " 'RUN_MERGE_AE': True,\n",
            " 'RUN_REGRESSION': True,\n",
            " 'SCALED_CSV': '/content/drive/My '\n",
            "               'Drive/Aurora_Prediction/MAG_L2_features_scaled.csv',\n",
            " 'SCALER_FINAL': '/content/drive/My '\n",
            "                 'Drive/Aurora_Prediction/mag_scaler_final.joblib',\n",
            " 'SCALER_STAGE1': '/content/drive/My '\n",
            "                  'Drive/Aurora_Prediction/mag_scaler_stage1.joblib'}\n",
            "\n",
            "Checking that the specified input paths exist:\n",
            "AE_FILE exists: True\n",
            "MAG_INPUT_FOLDER exists: True\n",
            "List MAG_INPUT_FOLDER (first 10 entries):\n",
            "['L1_MAG91N18P1AL10010309024025306040142676_N00_0000_000683_V00.nc', 'L1_MAG91N18P1AL10010409024025307031706836_N00_0000_000684_V00.nc', 'L1_MAG91N18P1AL10010509024025307040040402_N00_0000_000684_V00.nc', 'L1_MAG91N18P1AL10010609024025308031532211_N00_0000_000685_V00.nc', 'L1_MAG91N18P1AL10010709024025308035956252_N00_0000_000685_V00.nc', 'L1_MAG91N18P1AL10010809024025309031517596_N00_0000_000686_V00.nc', 'L1_MAG91N18P1AL10010909024025309035900499_N00_0000_000686_V00.nc', 'L1_MAG91N18P1AL10011009024025310031431084_N00_0000_000687_V00.nc', 'L1_MAG91N18P1AL10011109024025310035823570_N00_0000_000687_V00.nc', 'L1_MAG91N18P1AL10011209024025311031347831_N00_0000_000688_V00.nc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRE-PROCESSING AND FINAL CLEANUP"
      ],
      "metadata": {
        "id": "ZXJwgGqGUp6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4eFLkc7FUQhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aurora Electrojet Time parsing"
      ],
      "metadata": {
        "id": "TYGIxhypVsgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core Helper Functions:\n",
        "Implemented HDF5/.nc extraction, recursive dataset flattening, and CSV merging via combine_l2_nc_files.\n",
        "Added robust time parsing (infer_time_column, parse_time_column) and PCHIP interpolation helper pchip_interpolate_series.\n",
        "Included scaling helpers using StandardScaler and final_cleanup for log transforms and outlier removal.\n",
        "Outputs: reusable functions used by subsequent cells to build datasets and save scaler objects."
      ],
      "metadata": {
        "id": "FR5OcuFtkHyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROLL_WINDOWS = [\"5min\", \"10min\"]\n",
        "LAG_STEPS = [1, 2, 3]\n",
        "MIN_POINTS_FOR_INTERP = 3\n",
        "EPS = 1e-6\n",
        "\n",
        "def extract_from_group(group, prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Recursively extract datasets from an HDF5 group and return dict{name->1D-array}.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    for key in group.keys():\n",
        "        item = group[key]\n",
        "        full_name = f\"{prefix}/{key}\".strip(\"/\")\n",
        "        if isinstance(item, h5py.Dataset):\n",
        "            try:\n",
        "                arr = np.array(item[()]).flatten()\n",
        "                data[full_name] = arr\n",
        "            except Exception:\n",
        "                pass\n",
        "        elif isinstance(item, h5py.Group):\n",
        "            data.update(extract_from_group(item, full_name))\n",
        "    return data\n",
        "\n",
        "def combine_l2_nc_files(input_folder, output_csv, max_files=None):\n",
        "    \"\"\"\n",
        "    Read .nc files (prefers L2 but accepts L1), extract datasets, pad arrays and concatenate into CSV.\n",
        "    Returns path to saved CSV.\n",
        "    \"\"\"\n",
        "    all_frames = []\n",
        "    processed = 0\n",
        "    if not os.path.isdir(input_folder):\n",
        "        raise FileNotFoundError(f\"Input folder not found: {input_folder}\")\n",
        "    fnames = sorted(os.listdir(input_folder))\n",
        "    print(f\"Found {len(fnames)} files in {input_folder}\")\n",
        "    for fname in fnames:\n",
        "        if not fname.endswith(\".nc\"):\n",
        "            continue\n",
        "        if \"L2\" not in fname and \"L1\" not in fname:\n",
        "            continue\n",
        "        path = os.path.join(input_folder, fname)\n",
        "        try:\n",
        "            with h5py.File(path, \"r\") as nc:\n",
        "                extracted = extract_from_group(nc)\n",
        "            if not extracted:\n",
        "                print(\"  -> No readable datasets in\", fname)\n",
        "                continue\n",
        "            max_len = max(len(v) for v in extracted.values())\n",
        "            for k in list(extracted.keys()):\n",
        "                arr = extracted[k]\n",
        "                if len(arr) < max_len:\n",
        "                    extracted[k] = np.pad(arr, (0, max_len - len(arr)), constant_values=np.nan)\n",
        "            df = pd.DataFrame(extracted)\n",
        "            df[\"source_file\"] = fname\n",
        "            all_frames.append(df)\n",
        "            processed += 1\n",
        "            print(\"  Processed:\", fname, \"rows:\", df.shape[0])\n",
        "            if max_files and processed >= max_files:\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"  Error reading {fname}: {e}\")\n",
        "    if not all_frames:\n",
        "        raise RuntimeError(\"No readable .nc files found in folder.\")\n",
        "    combined = pd.concat(all_frames, ignore_index=True)\n",
        "    combined.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved combined CSV ({processed} files) -> {output_csv}\")\n",
        "    return output_csv\n",
        "\n",
        "def infer_time_column(df):\n",
        "    \"\"\"\n",
        "    Heuristic to find a time column among common names or epoch-like numeric fields.\n",
        "    \"\"\"\n",
        "    candidates = ['time', 'utc_time', 'TIME', 'Time', 'timestamp', 'epoch', 'epoch_for_cdf_mod', 'time_sec']\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # fallback: choose numeric column with large values (epoch)\n",
        "    for c in df.columns:\n",
        "        try:\n",
        "            vals = pd.to_numeric(df[c], errors='coerce').dropna()\n",
        "            if not vals.empty and abs(vals.max()) > 1e6:\n",
        "                return c\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(\"Could not find a time column automatically.\")\n",
        "\n",
        "def parse_time_column(df, time_col):\n",
        "    \"\"\"\n",
        "    Convert numeric epoch or string time to pandas datetime. Auto-detects units.\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(df[time_col], errors='coerce')\n",
        "    if s.dropna().empty:\n",
        "        return pd.to_datetime(df[time_col], errors='coerce')\n",
        "    max_val = s.dropna().abs().max()\n",
        "    if max_val > 1e16:\n",
        "        unit = \"ns\"\n",
        "    elif max_val > 1e12:\n",
        "        unit = \"ns\"\n",
        "    elif max_val > 1e9:\n",
        "        unit = \"ms\"\n",
        "    elif max_val > 1e6:\n",
        "        unit = \"s\"\n",
        "    else:\n",
        "        try:\n",
        "            return pd.to_datetime(df[time_col], errors='coerce')\n",
        "        except Exception:\n",
        "            unit = \"s\"\n",
        "    return pd.to_datetime(s, unit=unit, origin='unix', errors='coerce')\n",
        "\n",
        "def ensure_B_columns(df):\n",
        "    \"\"\"\n",
        "    Detect likely Bx, By, Bz column names from common variants.\n",
        "    Returns mapping {'Bx': colname or None, ...}\n",
        "    \"\"\"\n",
        "    candidates_map = {\n",
        "        'Bx': ['Bx', 'Bx_gse', 'Bx_gsm', 'B_x', 'bx', 'BX'],\n",
        "        'By': ['By', 'By_gse', 'By_gsm', 'B_y', 'by', 'BY'],\n",
        "        'Bz': ['Bz', 'Bz_gse', 'Bz_gsm', 'B_z', 'bz', 'BZ'],\n",
        "    }\n",
        "    found = {}\n",
        "    for k, cand in candidates_map.items():\n",
        "        hit = None\n",
        "        for c in cand:\n",
        "            if c in df.columns:\n",
        "                hit = c\n",
        "                break\n",
        "        found[k] = hit\n",
        "    return found\n",
        "\n",
        "def pchip_interpolate_series(time_numeric, y, target_time_numeric):\n",
        "    \"\"\"\n",
        "    Interpolate y sampled at time_numeric to target_time_numeric with PCHIP.\n",
        "    \"\"\"\n",
        "    mask = np.isfinite(y) & np.isfinite(time_numeric)\n",
        "    if mask.sum() < MIN_POINTS_FOR_INTERP:\n",
        "        return np.full_like(target_time_numeric, np.nan, dtype=float)\n",
        "    try:\n",
        "        f = PchipInterpolator(time_numeric[mask], y[mask], extrapolate=False)\n",
        "        return f(target_time_numeric)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return np.interp(target_time_numeric, time_numeric[mask], y[mask], left=np.nan, right=np.nan)\n",
        "        except Exception:\n",
        "            return np.full_like(target_time_numeric, np.nan, dtype=float)\n",
        "\n",
        "def run_feature_pipeline(input_csv, output_features_csv, scaler_path=None):\n",
        "    \"\"\"\n",
        "    Read combined CSV, parse time, interpolate core columns with PCHIP,\n",
        "    compute B_mag, dB_dt, rolling stats and lag features, save features CSV,\n",
        "    and produce a stage-1 scaled CSV.\n",
        "    \"\"\"\n",
        "    print(\"Loading combined CSV:\", input_csv)\n",
        "    raw = pd.read_csv(input_csv, dtype=str)\n",
        "    time_col = infer_time_column(raw)\n",
        "    print(\"Detected time column:\", time_col)\n",
        "    raw[time_col] = raw[time_col].replace('', np.nan)\n",
        "    dt = parse_time_column(raw, time_col)\n",
        "    raw['__datetime'] = dt\n",
        "    raw = raw.dropna(subset=['__datetime']).copy()\n",
        "    raw = raw.sort_values('__datetime').reset_index(drop=True)\n",
        "    raw.index = pd.DatetimeIndex(raw['__datetime'])\n",
        "\n",
        "    b_map = ensure_B_columns(raw)\n",
        "    print(\"Detected B columns mapping:\", b_map)\n",
        "\n",
        "    # coerce numeric columns\n",
        "    for c in raw.columns:\n",
        "        if c == '__datetime': continue\n",
        "        raw[c] = pd.to_numeric(raw[c], errors='coerce')\n",
        "\n",
        "    df_work = raw.copy()\n",
        "    time_numeric = df_work.index.view(np.int64) / 1e9  # seconds\n",
        "    target_time_numeric = time_numeric\n",
        "\n",
        "    primary_cols = [v for v in b_map.values() if v] + [c for c in ['x_gse','y_gse','z_gse','x_gsm','y_gsm','z_gsm'] if c in df_work.columns]\n",
        "    primary_cols = list(dict.fromkeys(primary_cols))\n",
        "    print(\"Columns considered for interpolation:\", primary_cols)\n",
        "\n",
        "    out = pd.DataFrame(index=df_work.index)\n",
        "    out['time'] = df_work['__datetime']\n",
        "    for col in primary_cols:\n",
        "        y = df_work[col].to_numpy(dtype=float)\n",
        "        out[col] = pchip_interpolate_series(time_numeric, y, target_time_numeric)\n",
        "\n",
        "    # pick canonical Bx,By,Bz\n",
        "    def select_first_available(choices):\n",
        "        for c in choices:\n",
        "            if c in out.columns and out[c].notna().any():\n",
        "                return out[c].astype(float)\n",
        "        return pd.Series(np.nan, index=out.index)\n",
        "\n",
        "    out['Bx'] = select_first_available(['Bx_gse','Bx_gsm','Bx', b_map.get('Bx')])\n",
        "    out['By'] = select_first_available(['By_gse','By_gsm','By', b_map.get('By')])\n",
        "    out['Bz'] = select_first_available(['Bz_gse','Bz_gsm','Bz', b_map.get('Bz')])\n",
        "\n",
        "    out['B_mag'] = np.sqrt(out['Bx']**2 + out['By']**2 + out['Bz']**2)\n",
        "\n",
        "    dt_seconds = out.index.to_series().diff().dt.total_seconds().fillna(0).to_numpy()\n",
        "    dB = out['B_mag'].diff().to_numpy()\n",
        "    dt_nonzero = dt_seconds.copy()\n",
        "    dt_nonzero[dt_nonzero == 0] = np.nan\n",
        "    out['dB_dt'] = dB / dt_nonzero\n",
        "\n",
        "    for rw in ROLL_WINDOWS:\n",
        "        out[f'B_mag_roll_mean_{rw}'] = out['B_mag'].rolling(rw, min_periods=1).mean()\n",
        "        out[f'B_mag_roll_std_{rw}'] = out['B_mag'].rolling(rw, min_periods=1).std().fillna(0)\n",
        "\n",
        "    for lag in LAG_STEPS:\n",
        "        out[f'B_mag_lag_{lag}'] = out['B_mag'].shift(lag)\n",
        "        out[f'dB_dt_lag_{lag}'] = out['dB_dt'].shift(lag)\n",
        "\n",
        "    if 'source_file' in df_work.columns:\n",
        "        out['source_file'] = df_work['source_file']\n",
        "\n",
        "    core = ['time','Bx','By','Bz','B_mag','dB_dt']\n",
        "    rest = [c for c in out.columns if c not in core]\n",
        "    out = out[core + rest]\n",
        "\n",
        "    out.to_csv(output_features_csv, index=False)\n",
        "    print(\"Saved features CSV ->\", output_features_csv)\n",
        "\n",
        "    numeric_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    scaler = None\n",
        "    if numeric_cols:\n",
        "        scaler = StandardScaler()\n",
        "        scaled = scaler.fit_transform(out[numeric_cols].fillna(0.0))\n",
        "        scaled_df = pd.DataFrame(scaled, columns=numeric_cols, index=out.index)\n",
        "        final_stage1 = pd.concat([out.drop(columns=numeric_cols), scaled_df], axis=1)\n",
        "        stage1_output = output_features_csv.replace(\".csv\", \"_scaled.csv\")\n",
        "        final_stage1.to_csv(stage1_output, index=False)\n",
        "        if scaler_path:\n",
        "            joblib.dump(scaler, scaler_path)\n",
        "            print(\"Saved stage-1 scaler ->\", scaler_path)\n",
        "        print(\"Saved stage-1 scaled CSV ->\", stage1_output)\n",
        "        return out, final_stage1, scaler\n",
        "    else:\n",
        "        out.to_csv(output_features_csv, index=False)\n",
        "        print(\"No numeric columns detected; saved features only.\")\n",
        "        return out, out.copy(), None\n",
        "\n",
        "def final_cleanup(stage1_df, final_csv, final_scaler_path):\n",
        "    \"\"\"\n",
        "    Apply log transforms, remove extreme outliers, final scaling, save final csv + scaler.\n",
        "    \"\"\"\n",
        "    df = stage1_df.copy()\n",
        "    log_features = [\n",
        "        \"B_mag\",\n",
        "        \"dB_dt\",\n",
        "        \"B_mag_roll_mean_5min\",\n",
        "        \"B_mag_roll_std_5min\",\n",
        "        \"B_mag_roll_mean_10min\",\n",
        "        \"B_mag_roll_std_10min\",\n",
        "    ]\n",
        "    for col in log_features:\n",
        "        if col in df.columns:\n",
        "            df[col + \"_log10\"] = np.log10(df[col].abs().fillna(0.0) + EPS)\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if numeric_cols:\n",
        "        for col in numeric_cols:\n",
        "            upper = df[col].quantile(0.999)\n",
        "            df = df[(df[col].isna()) | (df[col] <= upper)]\n",
        "\n",
        "    numeric_cols_after = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    scaler_final = None\n",
        "    if numeric_cols_after:\n",
        "        scaler_final = StandardScaler()\n",
        "        df[numeric_cols_after] = scaler_final.fit_transform(df[numeric_cols_after].fillna(0.0))\n",
        "        joblib.dump(scaler_final, final_scaler_path)\n",
        "        print(\"Saved final scaler ->\", final_scaler_path)\n",
        "\n",
        "    df.to_csv(final_csv, index=False)\n",
        "    print(\"Saved final ML-ready CSV ->\", final_csv)\n",
        "    return df, scaler_final"
      ],
      "metadata": {
        "id": "BjNYwVrlVKGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAG extraction"
      ],
      "metadata": {
        "id": "CbUT6-scVw85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAG .nc Extraction:\n",
        "Scanned the MAG folder and recursively extracted readable datasets from .nc files, padding variable-length arrays and concatenating into MAG_L2_raw_combined.csv.\n",
        "Processed a limited number of files in FAST_MODE for quick iteration, or all files when disabled.\n",
        "Preserved provenance via a source_file column.\n",
        "Note: large .nc files can be slow — run overnight for full extraction."
      ],
      "metadata": {
        "id": "kHyp_Be6kUUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RUN_EXTRACT:\", RUN_EXTRACT)\n",
        "print(\"MAG_INPUT_FOLDER:\", MAG_INPUT_FOLDER)\n",
        "print(\"RAW_COMBINED:\", RAW_COMBINED)\n",
        "\n",
        "if not RUN_EXTRACT:\n",
        "    print(\"RUN_EXTRACT is False — skipping extraction.\")\n",
        "else:\n",
        "    if not os.path.exists(MAG_INPUT_FOLDER):\n",
        "        raise FileNotFoundError(f\"MAG input folder not found: {MAG_INPUT_FOLDER}\")\n",
        "    # Use the helper function defined in Cell 4\n",
        "    max_files = 20 if FAST_MODE else None\n",
        "    try:\n",
        "        combined_path = combine_l2_nc_files(MAG_INPUT_FOLDER, RAW_COMBINED, max_files=max_files)\n",
        "        print(\"Combined CSV saved to:\", combined_path)\n",
        "        # show basic info\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv(combined_path, nrows=5)\n",
        "        print(\"Preview (first 5 rows):\")\n",
        "        print(df.head())\n",
        "        print(\"Columns:\", df.columns.tolist())\n",
        "        print(\"Combined file size (MB):\", os.path.getsize(combined_path)/1e6)\n",
        "    except Exception as e:\n",
        "        print(\"Error during extraction:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI5f4_J6V2mM",
        "outputId": "48f649da-a3db-4897-d8be-7c5b184810d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_EXTRACT: True\n",
            "MAG_INPUT_FOLDER: /content/drive/My Drive/Aurora_Prediction/MAG dataset/\n",
            "RAW_COMBINED: /content/drive/My Drive/Aurora_Prediction/MAG_L2_raw_combined.csv\n",
            "Found 26 files in /content/drive/My Drive/Aurora_Prediction/MAG dataset/\n",
            "  Processed: L1_MAG91N18P1AL10010309024025306040142676_N00_0000_000683_V00.nc rows: 337531\n",
            "  Processed: L1_MAG91N18P1AL10010409024025307031706836_N00_0000_000684_V00.nc rows: 337463\n",
            "  Processed: L1_MAG91N18P1AL10010509024025307040040402_N00_0000_000684_V00.nc rows: 337537\n",
            "  Processed: L1_MAG91N18P1AL10010609024025308031532211_N00_0000_000685_V00.nc rows: 337460\n",
            "  Processed: L1_MAG91N18P1AL10010709024025308035956252_N00_0000_000685_V00.nc rows: 337533\n",
            "  Processed: L1_MAG91N18P1AL10010809024025309031517596_N00_0000_000686_V00.nc rows: 337466\n",
            "  Processed: L1_MAG91N18P1AL10010909024025309035900499_N00_0000_000686_V00.nc rows: 337532\n",
            "  Processed: L1_MAG91N18P1AL10011009024025310031431084_N00_0000_000687_V00.nc rows: 337469\n",
            "  Processed: L1_MAG91N18P1AL10011109024025310035823570_N00_0000_000687_V00.nc rows: 337530\n",
            "  Processed: L1_MAG91N18P1AL10011209024025311031347831_N00_0000_000688_V00.nc rows: 337461\n",
            "  Processed: L1_MAG91N18P1AL10011309024025311035741894_N00_0000_000688_V00.nc rows: 337532\n",
            "  Processed: L1_MAG91N18P1AL10011409024025312031339064_N00_0000_000689_V00.nc rows: 337464\n",
            "  Processed: L1_MAG91N18P1AL10011509024025312035720334_N00_0000_000689_V00.nc rows: 337532\n",
            "  Processed: L1_MAG91N18P1AL10011609024025313031257651_N00_0000_000690_V00.nc rows: 337465\n",
            "  Processed: L1_MAG91N18P1AL10011709024025313035705576_N00_0000_000690_V00.nc rows: 337533\n",
            "  Processed: L1_MAG91N18P1AL10011809024025314031244987_N00_0000_000691_V00.nc rows: 337461\n",
            "  Processed: L1_MAG91N18P1AL10011909024025314035749697_N00_0000_000691_V00.nc rows: 337537\n",
            "  Processed: L1_MAG91N18P1AL10012009024025315031332880_N00_0000_000692_V00.nc rows: 337462\n",
            "  Processed: L1_MAG91N18P1AL10012109024025315035824623_N00_0000_000692_V00.nc rows: 337530\n",
            "  Processed: L1_MAG91N18P1AL10012209024025316031355183_N00_0000_000693_V00.nc rows: 337466\n",
            "Saved combined CSV (20 files) -> /content/drive/My Drive/Aurora_Prediction/MAG_L2_raw_combined.csv\n",
            "Combined CSV saved to: /content/drive/My Drive/Aurora_Prediction/MAG_L2_raw_combined.csv\n",
            "Preview (first 5 rows):\n",
            "   utc_time   Bx   By   Bz          time  B_yaw_mag1  B_roll_mag1  \\\n",
            "0       NaN  0.0  0.0  0.0  1.761998e+09  -16.341787     2.450443   \n",
            "1       NaN  0.0  0.0  0.0  1.761998e+09  -16.358885     2.450874   \n",
            "2       NaN  0.0  0.0  0.0  1.761998e+09  -16.359627     2.450892   \n",
            "3       NaN  0.0  0.0  0.0  1.761998e+09  -16.358526     2.368725   \n",
            "4       NaN  0.0  0.0  0.0  1.761998e+09  -16.362988     2.368837   \n",
            "\n",
            "   B_pitch_mag1  B_yaw_mag2  B_roll_mag2  ...  Bx2_gse_error  By2_gse_error  \\\n",
            "0     -0.882800  -12.533855   -30.508755  ...   9.969210e+36   9.969210e+36   \n",
            "1     -1.499435  -12.107467   -30.500381  ...   9.969210e+36   9.969210e+36   \n",
            "2     -1.526245  -12.134950   -30.496906  ...   9.969210e+36   9.969210e+36   \n",
            "3     -1.525967  -12.134950   -30.496906  ...   9.969210e+36   9.969210e+36   \n",
            "4     -1.686828  -12.134950   -30.496906  ...   9.969210e+36   9.969210e+36   \n",
            "\n",
            "   Bz2_gse_error  Bx1_gsm_error  By1_gsm_error  Bz1_gsm_error  Bx2_gsm_error  \\\n",
            "0   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   \n",
            "1   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   \n",
            "2   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   \n",
            "3   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   \n",
            "4   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   9.969210e+36   \n",
            "\n",
            "   By2_gsm_error  Bz2_gsm_error  \\\n",
            "0   9.969210e+36   9.969210e+36   \n",
            "1   9.969210e+36   9.969210e+36   \n",
            "2   9.969210e+36   9.969210e+36   \n",
            "3   9.969210e+36   9.969210e+36   \n",
            "4   9.969210e+36   9.969210e+36   \n",
            "\n",
            "                                         source_file  \n",
            "0  L1_MAG91N18P1AL10010309024025306040142676_N00_...  \n",
            "1  L1_MAG91N18P1AL10010309024025306040142676_N00_...  \n",
            "2  L1_MAG91N18P1AL10010309024025306040142676_N00_...  \n",
            "3  L1_MAG91N18P1AL10010309024025306040142676_N00_...  \n",
            "4  L1_MAG91N18P1AL10010309024025306040142676_N00_...  \n",
            "\n",
            "[5 rows x 42 columns]\n",
            "Columns: ['utc_time', 'Bx', 'By', 'Bz', 'time', 'B_yaw_mag1', 'B_roll_mag1', 'B_pitch_mag1', 'B_yaw_mag2', 'B_roll_mag2', 'B_pitch_mag2', 'Bx1_gse', 'By1_gse', 'Bz1_gse', 'Bx2_gse', 'By2_gse', 'Bz2_gse', 'Bx1_gsm', 'By1_gsm', 'Bz1_gsm', 'Bx2_gsm', 'By2_gsm', 'Bz2_gsm', 'B_yaw_mag1_error', 'B_roll_mag1_error', 'B_pitch_mag1_error', 'B_yaw_mag2_error', 'B_roll_mag2_error', 'B_pitch_mag2_error', 'Bx1_gse_error', 'By1_gse_error', 'Bz1_gse_error', 'Bx2_gse_error', 'By2_gse_error', 'Bz2_gse_error', 'Bx1_gsm_error', 'By1_gsm_error', 'Bz1_gsm_error', 'Bx2_gsm_error', 'By2_gsm_error', 'Bz2_gsm_error', 'source_file']\n",
            "Combined file size (MB): 3181.790585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE EXTRACTION STAGE(1)"
      ],
      "metadata": {
        "id": "05hbMua8Xpin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering (Stage 1):\n",
        "Ran run_feature_pipeline to parse time, interpolate core fields (Bx/By/Bz), compute B_mag and dB_dt, add rolling stats and lag features, and saved MAG_L2_features.csv.\n",
        "Produced a stage-1 scaled CSV (numeric cols standardized) and saved the stage-1 scaler as mag_scaler_stage1.joblib.\n",
        "This creates a consistent, time-indexed features table ready for final cleanup.\n",
        "Note: PCHIP interpolation preserves signal shape for magnetometer data."
      ],
      "metadata": {
        "id": "F55IA22WlyGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INPUT combined CSV expected at:\", RAW_COMBINED)\n",
        "if not os.path.exists(RAW_COMBINED):\n",
        "    raise FileNotFoundError(f\"Combined raw CSV not found at {RAW_COMBINED}. Run extraction (Cell 5) or set correct path.\")\n",
        "\n",
        "try:\n",
        "    # run_feature_pipeline is defined in Cell 4\n",
        "    features_unscaled, features_stage1_df, scaler_stage1 = run_feature_pipeline(RAW_COMBINED, FEATURES_CSV, SCALER_STAGE1)\n",
        "    print(\"Feature pipeline completed.\")\n",
        "    print(\"Unscaled features preview:\")\n",
        "    display(features_unscaled.head())\n",
        "    print(\"Stage-1 scaled preview:\")\n",
        "    display(features_stage1_df.head())\n",
        "    if scaler_stage1 is not None:\n",
        "        print(\"Stage-1 scaler saved to:\", SCALER_STAGE1)\n",
        "    # Also save a simple CSV copy to MODEL_DATASET_CSV placeholder (will be updated after final cleanup)\n",
        "    features_stage1_df.to_csv(MODEL_DATASET_CSV.replace(\".csv\",\"_stage1.csv\"), index=False)\n",
        "    print(\"Stage-1 dataset saved to:\", MODEL_DATASET_CSV.replace(\".csv\",\"_stage1.csv\"))\n",
        "except Exception as e:\n",
        "    print(\"Feature engineering failed:\")\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "sg_B8pdrXirX",
        "outputId": "deaddcd8-b107-47ec-dc6e-5f2caa49ba1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT combined CSV expected at: /content/drive/My Drive/Aurora_Prediction/MAG_L2_raw_combined.csv\n",
            "Loading combined CSV: /content/drive/My Drive/Aurora_Prediction/MAG_L2_raw_combined.csv\n",
            "Detected time column: time\n",
            "Detected B columns mapping: {'Bx': 'Bx', 'By': 'By', 'Bz': 'Bz'}\n",
            "Columns considered for interpolation: ['Bx', 'By', 'Bz']\n",
            "Saved features CSV -> /content/drive/My Drive/Aurora_Prediction/MAG_L2_features.csv\n",
            "Saved stage-1 scaler -> /content/drive/My Drive/Aurora_Prediction/mag_scaler_stage1.joblib\n",
            "Saved stage-1 scaled CSV -> /content/drive/My Drive/Aurora_Prediction/MAG_L2_features_scaled.csv\n",
            "Feature pipeline completed.\n",
            "Unscaled features preview:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                       time   Bx   By   Bz  \\\n",
              "__datetime                                                                   \n",
              "1970-01-21 09:26:38.401597295 1970-01-21 09:26:38.401597295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401725295 1970-01-21 09:26:38.401725295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401853295 1970-01-21 09:26:38.401853295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401981295 1970-01-21 09:26:38.401981295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.402109295 1970-01-21 09:26:38.402109295  0.0  0.0  0.0   \n",
              "\n",
              "                               B_mag  dB_dt  B_mag_roll_mean_5min  \\\n",
              "__datetime                                                          \n",
              "1970-01-21 09:26:38.401597295    0.0    NaN                   0.0   \n",
              "1970-01-21 09:26:38.401725295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401853295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401981295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.402109295    0.0    0.0                   0.0   \n",
              "\n",
              "                               B_mag_roll_std_5min  B_mag_roll_mean_10min  \\\n",
              "__datetime                                                                  \n",
              "1970-01-21 09:26:38.401597295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401725295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401853295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401981295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.402109295                  0.0                    0.0   \n",
              "\n",
              "                               B_mag_roll_std_10min  B_mag_lag_1  dB_dt_lag_1  \\\n",
              "__datetime                                                                      \n",
              "1970-01-21 09:26:38.401597295                   0.0          NaN          NaN   \n",
              "1970-01-21 09:26:38.401725295                   0.0          0.0          NaN   \n",
              "1970-01-21 09:26:38.401853295                   0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401981295                   0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.402109295                   0.0          0.0          0.0   \n",
              "\n",
              "                               B_mag_lag_2  dB_dt_lag_2  B_mag_lag_3  \\\n",
              "__datetime                                                             \n",
              "1970-01-21 09:26:38.401597295          NaN          NaN          NaN   \n",
              "1970-01-21 09:26:38.401725295          NaN          NaN          NaN   \n",
              "1970-01-21 09:26:38.401853295          0.0          NaN          NaN   \n",
              "1970-01-21 09:26:38.401981295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.402109295          0.0          0.0          0.0   \n",
              "\n",
              "                               dB_dt_lag_3  source_file  \n",
              "__datetime                                               \n",
              "1970-01-21 09:26:38.401597295          NaN          NaN  \n",
              "1970-01-21 09:26:38.401725295          NaN          NaN  \n",
              "1970-01-21 09:26:38.401853295          NaN          NaN  \n",
              "1970-01-21 09:26:38.401981295          NaN          NaN  \n",
              "1970-01-21 09:26:38.402109295          0.0          NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e34fa0b7-5bc9-4d35-938d-4d97ed8d56bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>Bx</th>\n",
              "      <th>By</th>\n",
              "      <th>Bz</th>\n",
              "      <th>B_mag</th>\n",
              "      <th>dB_dt</th>\n",
              "      <th>B_mag_roll_mean_5min</th>\n",
              "      <th>B_mag_roll_std_5min</th>\n",
              "      <th>B_mag_roll_mean_10min</th>\n",
              "      <th>B_mag_roll_std_10min</th>\n",
              "      <th>B_mag_lag_1</th>\n",
              "      <th>dB_dt_lag_1</th>\n",
              "      <th>B_mag_lag_2</th>\n",
              "      <th>dB_dt_lag_2</th>\n",
              "      <th>B_mag_lag_3</th>\n",
              "      <th>dB_dt_lag_3</th>\n",
              "      <th>source_file</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__datetime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401597295</th>\n",
              "      <td>1970-01-21 09:26:38.401597295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401725295</th>\n",
              "      <td>1970-01-21 09:26:38.401725295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401853295</th>\n",
              "      <td>1970-01-21 09:26:38.401853295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401981295</th>\n",
              "      <td>1970-01-21 09:26:38.401981295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.402109295</th>\n",
              "      <td>1970-01-21 09:26:38.402109295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e34fa0b7-5bc9-4d35-938d-4d97ed8d56bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e34fa0b7-5bc9-4d35-938d-4d97ed8d56bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e34fa0b7-5bc9-4d35-938d-4d97ed8d56bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-42be1fe2-9fce-40ae-a883-2351d051f95c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42be1fe2-9fce-40ae-a883-2351d051f95c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-42be1fe2-9fce-40ae-a883-2351d051f95c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    traceback\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"__datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-21 09:26:38.401597295\",\n        \"max\": \"1970-01-21 09:26:38.402109295\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1970-01-21 09:26:38.401725295\",\n          \"1970-01-21 09:26:38.402109295\",\n          \"1970-01-21 09:26:38.401853295\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-21 09:26:38.401597295\",\n        \"max\": \"1970-01-21 09:26:38.402109295\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1970-01-21 09:26:38.401725295\",\n          \"1970-01-21 09:26:38.402109295\",\n          \"1970-01-21 09:26:38.401853295\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"By\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bz\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_mean_5min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_std_5min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_mean_10min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_std_10min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_lag_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt_lag_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_lag_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt_lag_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_lag_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt_lag_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_file\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage-1 scaled preview:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                       time   Bx   By   Bz  \\\n",
              "__datetime                                                                   \n",
              "1970-01-21 09:26:38.401597295 1970-01-21 09:26:38.401597295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401725295 1970-01-21 09:26:38.401725295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401853295 1970-01-21 09:26:38.401853295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401981295 1970-01-21 09:26:38.401981295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.402109295 1970-01-21 09:26:38.402109295  0.0  0.0  0.0   \n",
              "\n",
              "                               B_mag  dB_dt  B_mag_roll_mean_5min  \\\n",
              "__datetime                                                          \n",
              "1970-01-21 09:26:38.401597295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401725295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401853295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401981295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.402109295    0.0    0.0                   0.0   \n",
              "\n",
              "                               B_mag_roll_std_5min  B_mag_roll_mean_10min  \\\n",
              "__datetime                                                                  \n",
              "1970-01-21 09:26:38.401597295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401725295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401853295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401981295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.402109295                  0.0                    0.0   \n",
              "\n",
              "                               B_mag_roll_std_10min  B_mag_lag_1  dB_dt_lag_1  \\\n",
              "__datetime                                                                      \n",
              "1970-01-21 09:26:38.401597295                   0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401725295                   0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401853295                   0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401981295                   0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.402109295                   0.0          0.0          0.0   \n",
              "\n",
              "                               B_mag_lag_2  dB_dt_lag_2  B_mag_lag_3  \\\n",
              "__datetime                                                             \n",
              "1970-01-21 09:26:38.401597295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401725295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401853295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401981295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.402109295          0.0          0.0          0.0   \n",
              "\n",
              "                               dB_dt_lag_3  source_file  \n",
              "__datetime                                               \n",
              "1970-01-21 09:26:38.401597295          0.0          0.0  \n",
              "1970-01-21 09:26:38.401725295          0.0          0.0  \n",
              "1970-01-21 09:26:38.401853295          0.0          0.0  \n",
              "1970-01-21 09:26:38.401981295          0.0          0.0  \n",
              "1970-01-21 09:26:38.402109295          0.0          0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f091ffa5-1b2c-4efe-8a37-7f316b041220\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>Bx</th>\n",
              "      <th>By</th>\n",
              "      <th>Bz</th>\n",
              "      <th>B_mag</th>\n",
              "      <th>dB_dt</th>\n",
              "      <th>B_mag_roll_mean_5min</th>\n",
              "      <th>B_mag_roll_std_5min</th>\n",
              "      <th>B_mag_roll_mean_10min</th>\n",
              "      <th>B_mag_roll_std_10min</th>\n",
              "      <th>B_mag_lag_1</th>\n",
              "      <th>dB_dt_lag_1</th>\n",
              "      <th>B_mag_lag_2</th>\n",
              "      <th>dB_dt_lag_2</th>\n",
              "      <th>B_mag_lag_3</th>\n",
              "      <th>dB_dt_lag_3</th>\n",
              "      <th>source_file</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__datetime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401597295</th>\n",
              "      <td>1970-01-21 09:26:38.401597295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401725295</th>\n",
              "      <td>1970-01-21 09:26:38.401725295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401853295</th>\n",
              "      <td>1970-01-21 09:26:38.401853295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401981295</th>\n",
              "      <td>1970-01-21 09:26:38.401981295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.402109295</th>\n",
              "      <td>1970-01-21 09:26:38.402109295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f091ffa5-1b2c-4efe-8a37-7f316b041220')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f091ffa5-1b2c-4efe-8a37-7f316b041220 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f091ffa5-1b2c-4efe-8a37-7f316b041220');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e9261b2a-188d-4b0c-807b-5d2b1d18880e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9261b2a-188d-4b0c-807b-5d2b1d18880e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e9261b2a-188d-4b0c-807b-5d2b1d18880e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    traceback\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"__datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-21 09:26:38.401597295\",\n        \"max\": \"1970-01-21 09:26:38.402109295\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1970-01-21 09:26:38.401725295\",\n          \"1970-01-21 09:26:38.402109295\",\n          \"1970-01-21 09:26:38.401853295\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-21 09:26:38.401597295\",\n        \"max\": \"1970-01-21 09:26:38.402109295\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1970-01-21 09:26:38.401725295\",\n          \"1970-01-21 09:26:38.402109295\",\n          \"1970-01-21 09:26:38.401853295\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"By\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bz\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_mean_5min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_std_5min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_mean_10min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_roll_std_10min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_lag_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt_lag_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_lag_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt_lag_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B_mag_lag_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dB_dt_lag_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_file\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage-1 scaler saved to: /content/drive/My Drive/Aurora_Prediction/mag_scaler_stage1.joblib\n",
            "Stage-1 dataset saved to: /content/drive/My Drive/Aurora_Prediction/MAG_L2_model_dataset_stage1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Cleanup (Stage 2):\n",
        "Applied log10 transforms to selected features, removed extreme outliers (>99.9 percentile), and ran a second StandardScaler to produce MAG_L2_final_features.csv.\n",
        "Saved the final scaler as mag_scaler_final.joblib and exported MAG_L2_model_dataset.csv for modeling.\n",
        "Result: ML-ready dataset with stabilized distributions and reproducible scaling.\n",
        "Tip: keep scalers with your models for inference consistency."
      ],
      "metadata": {
        "id": "D-U_vd1sl5rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Looking for features_unscaled in memory or FEATURES_CSV on disk...\")\n",
        "if 'features_unscaled' in globals():\n",
        "    stage1_unscaled = features_unscaled\n",
        "    print(\"Using `features_unscaled` from memory.\")\n",
        "elif os.path.exists(FEATURES_CSV):\n",
        "    stage1_unscaled = pd.read_csv(FEATURES_CSV, parse_dates=['time'], infer_datetime_format=True)\n",
        "    print(\"Loaded FEATURES_CSV from disk:\", FEATURES_CSV)\n",
        "else:\n",
        "    raise FileNotFoundError(\"No `features_unscaled` in memory and FEATURES_CSV missing. Run Cell 6 first.\")\n",
        "\n",
        "try:\n",
        "    final_df, scaler_final = final_cleanup(stage1_unscaled, FINAL_CSV, SCALER_FINAL)\n",
        "    # Also save as MODEL_DATASET_CSV for downstream cells\n",
        "    final_df.to_csv(MODEL_DATASET_CSV, index=False)\n",
        "    print(\"Final ML-ready dataset saved to:\", FINAL_CSV)\n",
        "    print(\"Also copied to MODEL_DATASET_CSV:\", MODEL_DATASET_CSV)\n",
        "    if scaler_final is not None:\n",
        "        print(\"Final scaler saved to:\", SCALER_FINAL)\n",
        "    print(\"\\nFinal dataset shape:\", final_df.shape)\n",
        "    display(final_df.head())\n",
        "except Exception as e:\n",
        "    print(\"Final cleanup failed:\")\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "iinTchIsZ7wP",
        "outputId": "0c821ea7-595f-4c13-a042-3f039f6844b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for features_unscaled in memory or FEATURES_CSV on disk...\n",
            "Using `features_unscaled` from memory.\n",
            "Saved final scaler -> /content/drive/My Drive/Aurora_Prediction/mag_scaler_final.joblib\n",
            "Saved final ML-ready CSV -> /content/drive/My Drive/Aurora_Prediction/MAG_L2_final_features.csv\n",
            "Final ML-ready dataset saved to: /content/drive/My Drive/Aurora_Prediction/MAG_L2_final_features.csv\n",
            "Also copied to MODEL_DATASET_CSV: /content/drive/My Drive/Aurora_Prediction/MAG_L2_model_dataset.csv\n",
            "Final scaler saved to: /content/drive/My Drive/Aurora_Prediction/mag_scaler_final.joblib\n",
            "\n",
            "Final dataset shape: (6749964, 23)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                       time   Bx   By   Bz  \\\n",
              "__datetime                                                                   \n",
              "1970-01-21 09:26:38.401597295 1970-01-21 09:26:38.401597295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401725295 1970-01-21 09:26:38.401725295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401853295 1970-01-21 09:26:38.401853295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.401981295 1970-01-21 09:26:38.401981295  0.0  0.0  0.0   \n",
              "1970-01-21 09:26:38.402109295 1970-01-21 09:26:38.402109295  0.0  0.0  0.0   \n",
              "\n",
              "                               B_mag  dB_dt  B_mag_roll_mean_5min  \\\n",
              "__datetime                                                          \n",
              "1970-01-21 09:26:38.401597295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401725295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401853295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.401981295    0.0    0.0                   0.0   \n",
              "1970-01-21 09:26:38.402109295    0.0    0.0                   0.0   \n",
              "\n",
              "                               B_mag_roll_std_5min  B_mag_roll_mean_10min  \\\n",
              "__datetime                                                                  \n",
              "1970-01-21 09:26:38.401597295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401725295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401853295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.401981295                  0.0                    0.0   \n",
              "1970-01-21 09:26:38.402109295                  0.0                    0.0   \n",
              "\n",
              "                               B_mag_roll_std_10min  ...  dB_dt_lag_2  \\\n",
              "__datetime                                           ...                \n",
              "1970-01-21 09:26:38.401597295                   0.0  ...          0.0   \n",
              "1970-01-21 09:26:38.401725295                   0.0  ...          0.0   \n",
              "1970-01-21 09:26:38.401853295                   0.0  ...          0.0   \n",
              "1970-01-21 09:26:38.401981295                   0.0  ...          0.0   \n",
              "1970-01-21 09:26:38.402109295                   0.0  ...          0.0   \n",
              "\n",
              "                               B_mag_lag_3  dB_dt_lag_3  source_file  \\\n",
              "__datetime                                                             \n",
              "1970-01-21 09:26:38.401597295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401725295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401853295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.401981295          0.0          0.0          0.0   \n",
              "1970-01-21 09:26:38.402109295          0.0          0.0          0.0   \n",
              "\n",
              "                               B_mag_log10  dB_dt_log10  \\\n",
              "__datetime                                                \n",
              "1970-01-21 09:26:38.401597295          0.0          0.0   \n",
              "1970-01-21 09:26:38.401725295          0.0          0.0   \n",
              "1970-01-21 09:26:38.401853295          0.0          0.0   \n",
              "1970-01-21 09:26:38.401981295          0.0          0.0   \n",
              "1970-01-21 09:26:38.402109295          0.0          0.0   \n",
              "\n",
              "                               B_mag_roll_mean_5min_log10  \\\n",
              "__datetime                                                  \n",
              "1970-01-21 09:26:38.401597295                         0.0   \n",
              "1970-01-21 09:26:38.401725295                         0.0   \n",
              "1970-01-21 09:26:38.401853295                         0.0   \n",
              "1970-01-21 09:26:38.401981295                         0.0   \n",
              "1970-01-21 09:26:38.402109295                         0.0   \n",
              "\n",
              "                               B_mag_roll_std_5min_log10  \\\n",
              "__datetime                                                 \n",
              "1970-01-21 09:26:38.401597295                        0.0   \n",
              "1970-01-21 09:26:38.401725295                        0.0   \n",
              "1970-01-21 09:26:38.401853295                        0.0   \n",
              "1970-01-21 09:26:38.401981295                        0.0   \n",
              "1970-01-21 09:26:38.402109295                        0.0   \n",
              "\n",
              "                               B_mag_roll_mean_10min_log10  \\\n",
              "__datetime                                                   \n",
              "1970-01-21 09:26:38.401597295                          0.0   \n",
              "1970-01-21 09:26:38.401725295                          0.0   \n",
              "1970-01-21 09:26:38.401853295                          0.0   \n",
              "1970-01-21 09:26:38.401981295                          0.0   \n",
              "1970-01-21 09:26:38.402109295                          0.0   \n",
              "\n",
              "                               B_mag_roll_std_10min_log10  \n",
              "__datetime                                                 \n",
              "1970-01-21 09:26:38.401597295                         0.0  \n",
              "1970-01-21 09:26:38.401725295                         0.0  \n",
              "1970-01-21 09:26:38.401853295                         0.0  \n",
              "1970-01-21 09:26:38.401981295                         0.0  \n",
              "1970-01-21 09:26:38.402109295                         0.0  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1eca660d-d0a9-4c25-9371-a67328d2c29f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>Bx</th>\n",
              "      <th>By</th>\n",
              "      <th>Bz</th>\n",
              "      <th>B_mag</th>\n",
              "      <th>dB_dt</th>\n",
              "      <th>B_mag_roll_mean_5min</th>\n",
              "      <th>B_mag_roll_std_5min</th>\n",
              "      <th>B_mag_roll_mean_10min</th>\n",
              "      <th>B_mag_roll_std_10min</th>\n",
              "      <th>...</th>\n",
              "      <th>dB_dt_lag_2</th>\n",
              "      <th>B_mag_lag_3</th>\n",
              "      <th>dB_dt_lag_3</th>\n",
              "      <th>source_file</th>\n",
              "      <th>B_mag_log10</th>\n",
              "      <th>dB_dt_log10</th>\n",
              "      <th>B_mag_roll_mean_5min_log10</th>\n",
              "      <th>B_mag_roll_std_5min_log10</th>\n",
              "      <th>B_mag_roll_mean_10min_log10</th>\n",
              "      <th>B_mag_roll_std_10min_log10</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__datetime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401597295</th>\n",
              "      <td>1970-01-21 09:26:38.401597295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401725295</th>\n",
              "      <td>1970-01-21 09:26:38.401725295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401853295</th>\n",
              "      <td>1970-01-21 09:26:38.401853295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.401981295</th>\n",
              "      <td>1970-01-21 09:26:38.401981295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970-01-21 09:26:38.402109295</th>\n",
              "      <td>1970-01-21 09:26:38.402109295</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1eca660d-d0a9-4c25-9371-a67328d2c29f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1eca660d-d0a9-4c25-9371-a67328d2c29f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1eca660d-d0a9-4c25-9371-a67328d2c29f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-04a4d24a-bb4d-426f-b340-906078709d69\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04a4d24a-bb4d-426f-b340-906078709d69')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-04a4d24a-bb4d-426f-b340-906078709d69 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Spike Cleaning & Recompute Features:\n",
        "Implemented rolling-MAD spike detection + local-median replacement and a 5-point median filter on Bx/By/Bz to remove high-frequency sensor spikes.\n",
        "Recomputed B_mag, dB_dt, rolling means/stds, lag features, and log transforms, then re-scaled selected columns and saved the cleaned MAG_L2_model_dataset.csv.\n",
        "Saved an additional scaler mag_spike_clean_scaler.joblib for the cleaned components.\n",
        "Note: cleaning reduces false dB/dt spikes that hurt model training."
      ],
      "metadata": {
        "id": "WzQEjFFPmDsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Advanced spike cleaning, recompute B_mag/dB_dt, rolling features, lags, and scale\n",
        "import pandas as pd, numpy as np, os, traceback\n",
        "from scipy.stats import median_abs_deviation\n",
        "from scipy.signal import medfilt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "print(\"Loading model dataset:\", MODEL_DATASET_CSV)\n",
        "if not os.path.exists(MODEL_DATASET_CSV):\n",
        "    raise FileNotFoundError(f\"MODEL_DATASET_CSV not found: {MODEL_DATASET_CSV}. Run previous cells first.\")\n",
        "\n",
        "# Load (this should be the final CSV from cleanup)\n",
        "df = pd.read_csv(MODEL_DATASET_CSV, parse_dates=['time'], infer_datetime_format=True, low_memory=False)\n",
        "print(\"Loaded shape:\", df.shape)\n",
        "\n",
        "# Ensure time present\n",
        "if 'time' not in df.columns:\n",
        "    # If no `time` column, try to detect datetime-like column\n",
        "    for c in df.columns:\n",
        "        if np.issubdtype(df[c].dtype, np.datetime64):\n",
        "            df = df.rename(columns={c: 'time'})\n",
        "            break\n",
        "    if 'time' not in df.columns:\n",
        "        # create index-based datetime as fallback\n",
        "        df['time'] = pd.date_range(start='1970-01-01', periods=len(df), freq='S')\n",
        "\n",
        "df = df.sort_values('time').reset_index(drop=True)\n",
        "\n",
        "# Spike cleaning function (rolling MAD + median filter)\n",
        "def clean_series(series, window_size=15, n_sigmas=3):\n",
        "    s = series.copy().astype(float).fillna(0.0).to_numpy()\n",
        "    cleaned = s.copy()\n",
        "    L = window_size // 2\n",
        "    k = 1.4826  # MAD -> std approx\n",
        "    N = len(s)\n",
        "    # avoid extremes at edges by leaving first/last L points unchanged except median filter after\n",
        "    for i in range(L, N - L):\n",
        "        window = s[i-L:i+L+1]\n",
        "        med = np.median(window)\n",
        "        mad = median_abs_deviation(window, scale='normal')  # returns scaled MAD by default? ensure fallback below\n",
        "        # robust fallback:\n",
        "        if np.isnan(mad) or mad == 0:\n",
        "            mad = np.median(np.abs(window - med)) or 1e-6\n",
        "        threshold = n_sigmas * k * mad\n",
        "        if abs(s[i] - med) > threshold:\n",
        "            cleaned[i] = med\n",
        "    # apply small median filter at the end\n",
        "    try:\n",
        "        cleaned = medfilt(cleaned, kernel_size=5)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return pd.Series(cleaned, index=series.index)\n",
        "\n",
        "# Apply cleaning to Bx/By/Bz if present\n",
        "for comp in ['Bx','By','Bz']:\n",
        "    if comp in df.columns:\n",
        "        print(\"Cleaning:\", comp)\n",
        "        df[f'{comp}_clean'] = clean_series(pd.to_numeric(df[comp], errors='coerce').fillna(0.0))\n",
        "    else:\n",
        "        print(\"Column missing, skipping:\", comp)\n",
        "\n",
        "# Recompute B_mag from cleaned components (prefer cleaned if available)\n",
        "if all(c in df.columns for c in ['Bx_clean','By_clean','Bz_clean']):\n",
        "    df['B_mag'] = np.sqrt(df['Bx_clean']**2 + df['By_clean']**2 + df['Bz_clean']**2)\n",
        "else:\n",
        "    # fallback to raw columns if cleaned not available\n",
        "    df['B_mag'] = np.sqrt(df.get('Bx',0)**2 + df.get('By',0)**2 + df.get('Bz',0)**2)\n",
        "\n",
        "# Compute dB/dt as first difference (preserving time delta in seconds if time index is datetime)\n",
        "df['dB_dt'] = df['B_mag'].diff()\n",
        "if np.issubdtype(df['time'].dtype, np.datetime64):\n",
        "    dt_seconds = df['time'].diff().dt.total_seconds().fillna(0)\n",
        "    dt_nonzero = dt_seconds.replace(0, np.nan)\n",
        "    df['dB_dt'] = df['B_mag'].diff() / dt_nonzero\n",
        "else:\n",
        "    # sample-based diff (fallback)\n",
        "    df['dB_dt'] = df['B_mag'].diff()\n",
        "\n",
        "# Interpolate small gaps in numeric columns\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "df[num_cols] = df[num_cols].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "# Rolling statistics (use sample windows 5 and 10)\n",
        "for w in [5, 10]:\n",
        "    df[f'B_mag_mean_{w}'] = df['B_mag'].rolling(window=w, min_periods=1).mean()\n",
        "    df[f'B_mag_std_{w}'] = df['B_mag'].rolling(window=w, min_periods=1).std().fillna(0.0)\n",
        "\n",
        "# Lag features\n",
        "for lag in [1,2,3]:\n",
        "    df[f'B_mag_lag_{lag}'] = df['B_mag'].shift(lag)\n",
        "    df[f'dB_dt_lag_{lag}'] = df['dB_dt'].shift(lag)\n",
        "\n",
        "# Log transforms\n",
        "df['B_mag_log10'] = np.log10(np.abs(df['B_mag'].fillna(0.0)) + 1e-12)\n",
        "df['dB_dt_log10'] = np.log10(np.abs(df['dB_dt'].fillna(0.0)) + 1e-12)\n",
        "\n",
        "# Drop rows with NaNs introduced by lagging\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "print(\"After cleaning/features shape:\", df.shape)\n",
        "\n",
        "# Scale selected columns for modeling (keep scaler)\n",
        "scale_cols = [c for c in ['Bx_clean','By_clean','Bz_clean','B_mag','dB_dt'] if c in df.columns]\n",
        "scaler = None\n",
        "if scale_cols:\n",
        "    scaler = StandardScaler()\n",
        "    df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
        "    # save scaler\n",
        "    scaler_path = os.path.join(MODELS_DIR, \"scalers\")\n",
        "    os.makedirs(scaler_path, exist_ok=True)\n",
        "    joblib.dump(scaler, os.path.join(scaler_path, \"mag_spike_clean_scaler.joblib\"))\n",
        "    print(\"Saved spike-clean scaler to:\", os.path.join(scaler_path, \"mag_spike_clean_scaler.joblib\"))\n",
        "\n",
        "# Save processed dataset for modeling\n",
        "df.to_csv(MODEL_DATASET_CSV, index=False)\n",
        "print(\"Saved cleaned model dataset to:\", MODEL_DATASET_CSV)\n",
        "print(\"Preview:\")\n",
        "display(df.head())\n",
        "\n",
        "# If FAST_MODE, optionally write a small sample for fast prototyping\n",
        "if FAST_MODE:\n",
        "    sample_path = MODEL_DATASET_CSV.replace(\".csv\",\"_fast_sample.csv\")\n",
        "    df.head(5000).to_csv(sample_path, index=False)\n",
        "    print(\"Saved FAST sample to:\", sample_path)\n",
        "\n",
        "# Done\n",
        "print(\"Advanced spike cleaning completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "mQ3m1fc-d_B2",
        "outputId": "4818524c-5305-4063-867f-40d044125a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model dataset: /content/drive/My Drive/Aurora_Prediction/MAG_L2_model_dataset.csv\n",
            "Loaded shape: (6749964, 23)\n",
            "Cleaning: Bx\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-415795559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaning:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{comp}_clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column missing, skipping:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-415795559.py\u001b[0m in \u001b[0;36mclean_series\u001b[0;34m(series, window_size, n_sigmas)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedian_abs_deviation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# returns scaled MAD by default? ensure fallback below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# robust fallback:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmad\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36mmedian_abs_deviation\u001b[0;34m(x, axis, center, scale, nan_policy)\u001b[0m\n\u001b[1;32m   3392\u001b[0m             \u001b[0;31m# keepdims=True was used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m             \u001b[0mmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3394\u001b[0;31m             \u001b[0mmad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmad\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REGRESSION TRAINING"
      ],
      "metadata": {
        "id": "kntpR0eqfazF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Training (RF, XGBoost, LSTM):\n",
        "Built target B_mag_target = B_mag(t+1), split timewise (80/20), trained Random Forest and XGBoost regressors, and saved them under regression.\n",
        "Prepared sequences and trained an LSTM (when TensorFlow and enough data present), with early stopping and reduced epochs in FAST_MODE.\n",
        "Saved models: random_forest_model.joblib, xgboost_model.joblib, and lstm_model.keras.\n",
        "Outputs: model comparison CSV with RMSE/MAE/R² for quick model selection."
      ],
      "metadata": {
        "id": "Xlpz5uo8mJyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Regression pipeline - RandomForest, XGBoost, LSTM\n",
        "import os, numpy as np, pandas as pd, joblib, traceback\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "print(\"Loading model dataset:\", MODEL_DATASET_CSV)\n",
        "if not os.path.exists(MODEL_DATASET_CSV):\n",
        "    raise FileNotFoundError(f\"MODEL_DATASET_CSV not found: {MODEL_DATASET_CSV}. Run previous cells first.\")\n",
        "\n",
        "df = pd.read_csv(MODEL_DATASET_CSV, low_memory=False)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "# Create target: B_mag at t+1\n",
        "if 'B_mag' not in df.columns:\n",
        "    raise KeyError(\"B_mag column not found in MODEL_DATASET_CSV\")\n",
        "\n",
        "df['B_mag_target'] = df['B_mag'].shift(-1)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "print(\"After creating target, shape:\", df.shape)\n",
        "\n",
        "# Select numeric features excluding time/source/target\n",
        "exclude_cols = ['time','B_mag_target','source_file'] if 'time' in df.columns else ['B_mag_target','source_file']\n",
        "feature_cols = [c for c in df.columns if c not in exclude_cols and np.issubdtype(df[c].dtype, np.number)]\n",
        "print(f\"Using {len(feature_cols)} features for regression.\")\n",
        "\n",
        "X = df[feature_cols].values\n",
        "y = df['B_mag_target'].values\n",
        "\n",
        "# Train/test split (time-series)\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# ---------------- Random Forest (baseline) ----------------\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100 if FAST_MODE else 300,\n",
        "    max_depth=20 if FAST_MODE else 30,\n",
        "    min_samples_leaf=4, random_state=42, n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
        "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
        "rf_r2 = r2_score(y_test, rf_pred)\n",
        "print(f\"RF - RMSE: {rf_rmse:.4f}, MAE: {rf_mae:.4f}, R2: {rf_r2:.4f}\")\n",
        "joblib.dump(rf, os.path.join(MODELS_DIR, 'regression', 'random_forest_model.joblib'))\n",
        "print(\"Saved RF model.\")\n",
        "\n",
        "# ---------------- XGBoost ----------------\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=100 if FAST_MODE else 300,\n",
        "    max_depth=6 if FAST_MODE else 10,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8, colsample_bytree=0.8,\n",
        "    random_state=42, n_jobs=-1, verbosity=0\n",
        ")\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
        "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
        "xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "print(f\"XGB - RMSE: {xgb_rmse:.4f}, MAE: {xgb_mae:.4f}, R2: {xgb_r2:.4f}\")\n",
        "joblib.dump(xgb, os.path.join(MODELS_DIR, 'regression', 'xgboost_model.joblib'))\n",
        "print(\"Saved XGBoost model.\")\n",
        "\n",
        "# ---------------- LSTM ----------------\n",
        "print(\"\\nPreparing sequences for LSTM...\")\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "except Exception as e:\n",
        "    tf = None\n",
        "    print(\"TensorFlow import failed:\", e)\n",
        "\n",
        "SEQ_LEN = 10\n",
        "def create_sequences(X, y, seq_len):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i+seq_len])\n",
        "        ys.append(y[i+seq_len])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
        "print(\"LSTM shapes:\", X_train_seq.shape, X_test_seq.shape)\n",
        "\n",
        "if tf is not None and X_train_seq.size > 0:\n",
        "    print(\"\\nBuilding LSTM model...\")\n",
        "    lstm = keras.Sequential([\n",
        "        layers.LSTM(64, activation='tanh', return_sequences=True, input_shape=(SEQ_LEN, X_train.shape[1])),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(32, activation='tanh'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    lstm.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
        "    early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3 if FAST_MODE else 10, restore_best_weights=True)\n",
        "    history = lstm.fit(X_train_seq, y_train_seq, validation_split=0.2, epochs=5 if FAST_MODE else 50, batch_size=32, callbacks=[early], verbose=1)\n",
        "    lstm_pred = lstm.predict(X_test_seq).flatten()\n",
        "    lstm_rmse = np.sqrt(mean_squared_error(y_test_seq, lstm_pred))\n",
        "    lstm_mae = mean_absolute_error(y_test_seq, lstm_pred)\n",
        "    lstm_r2 = r2_score(y_test_seq, lstm_pred)\n",
        "    print(f\"LSTM - RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, R2: {lstm_r2:.4f}\")\n",
        "    lstm.save(os.path.join(MODELS_DIR, 'regression', 'lstm_model.keras'))\n",
        "    print(\"Saved LSTM model.\")\n",
        "else:\n",
        "    print(\"Skipping LSTM training: TensorFlow missing or insufficient data.\")\n",
        "    lstm_rmse = lstm_mae = lstm_r2 = None\n",
        "\n",
        "# ---------------- Summary ----------------\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['RandomForest','XGBoost','LSTM'],\n",
        "    'RMSE': [rf_rmse, xgb_rmse, lstm_rmse],\n",
        "    'MAE':  [rf_mae, xgb_mae, lstm_mae],\n",
        "    'R2':   [rf_r2, xgb_r2, lstm_r2]\n",
        "})\n",
        "out_csv = os.path.join(MODELS_DIR, 'regression', 'model_comparison.csv')\n",
        "results.to_csv(out_csv, index=False)\n",
        "print(\"\\nModel comparison saved to:\", out_csv)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "2O44LwmIfjjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auroral electrojet merging and training"
      ],
      "metadata": {
        "id": "-_QC7rc7fvJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AE Merge & Labeling:\n",
        "Correctly parsed AE_dataset.txt (handles zero-padded HHMMSS), computed averaged AE = mean(AE1..AE4), built datetime, and aligned AE to MAG via pd.merge_asof with a 3‑minute tolerance.\n",
        "Created binary labels aurora_label = (AE > 20) and saved Aurora_Classification_dataset.csv, Aurora_Train.csv, and Aurora_Test.csv.\n",
        "Ensured stratified train/test split where possible to preserve class balance.\n",
        "Important fix: HHMMSS parsing avoids hour/minute/second misalignment that would corrupt labels."
      ],
      "metadata": {
        "id": "XWKnD77FmWwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: AE merge & labeling - parse AE file, align with MAG, create labels and train/test CSVs\n",
        "import os, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"AE file:\", AE_FILE)\n",
        "print(\"Model dataset (MAG):\", MODEL_DATASET_CSV)\n",
        "\n",
        "# --- 1) load MAG dataset ---\n",
        "if not os.path.exists(MODEL_DATASET_CSV):\n",
        "    raise FileNotFoundError(f\"Model dataset not found: {MODEL_DATASET_CSV}. Run previous cells first.\")\n",
        "mag = pd.read_csv(MODEL_DATASET_CSV, low_memory=False)\n",
        "# ensure datetime column\n",
        "if 'time' in mag.columns:\n",
        "    mag['datetime'] = pd.to_datetime(mag['time'], errors='coerce')\n",
        "else:\n",
        "    # attempt to find a datetime-like column\n",
        "    for c in mag.columns:\n",
        "        if np.issubdtype(mag[c].dtype, np.datetime64):\n",
        "            mag['datetime'] = mag[c]\n",
        "            break\n",
        "    if 'datetime' not in mag.columns:\n",
        "        mag['datetime'] = pd.date_range('1970-01-01', periods=len(mag), freq='S')\n",
        "mag = mag.sort_values('datetime').reset_index(drop=True)\n",
        "print(\"MAG records:\", len(mag))\n",
        "\n",
        "# --- 2) load AE dataset with correct HHMMSS parsing ---\n",
        "if not os.path.exists(AE_FILE):\n",
        "    raise FileNotFoundError(f\"AE file not found: {AE_FILE}\")\n",
        "\n",
        "# AE file has no header, whitespace-separated. Columns: year month day hhmmss AE1 AE2 AE3 AE4\n",
        "ae = pd.read_csv(AE_FILE, sep=r'\\s+', header=None, names=['year','month','day','hhmmss','AE1','AE2','AE3','AE4'], dtype={'hhmmss':str})\n",
        "# normalize hhmmss strings (pad to length 6)\n",
        "ae['hhmmss'] = ae['hhmmss'].str.zfill(6)\n",
        "\n",
        "# extract hour, minute, second\n",
        "ae['hour'] = ae['hhmmss'].str.slice(0,2).astype(int)\n",
        "ae['minute'] = ae['hhmmss'].str.slice(2,4).astype(int)\n",
        "ae['second'] = ae['hhmmss'].str.slice(4,6).astype(int)\n",
        "\n",
        "ae['datetime'] = pd.to_datetime(ae[['year','month','day','hour','minute','second']], errors='coerce')\n",
        "ae = ae.sort_values('datetime').reset_index(drop=True)\n",
        "print(\"AE records:\", len(ae))\n",
        "print(\"AE datetime range:\", ae['datetime'].min(), \"->\", ae['datetime'].max())\n",
        "\n",
        "# --- 3) compute AE average and basic QC ---\n",
        "ae['AE'] = ae[['AE1','AE2','AE3','AE4']].apply(pd.to_numeric, errors='coerce').mean(axis=1)\n",
        "print(\"AE stats:\")\n",
        "print(ae['AE'].describe())\n",
        "\n",
        "# Optional: drop NA datetimes\n",
        "ae = ae.dropna(subset=['datetime']).reset_index(drop=True)\n",
        "\n",
        "# --- 4) align AE to MAG using merge_asof (nearest) ---\n",
        "# ensure both sorted\n",
        "mag_sorted = mag.sort_values('datetime').reset_index(drop=True)\n",
        "ae_sorted = ae[['datetime','AE']].sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "# choose a tolerance: since AE sampling ~150s (2.5min), allow 3 minutes\n",
        "tolerance = pd.Timedelta('3min')\n",
        "merged = pd.merge_asof(mag_sorted, ae_sorted, on='datetime', direction='nearest', tolerance=tolerance)\n",
        "\n",
        "# drop rows where AE could not be matched\n",
        "matched = merged.dropna(subset=['AE']).reset_index(drop=True)\n",
        "print(\"Aligned records (within tolerance):\", len(matched), \"of\", len(mag_sorted))\n",
        "\n",
        "# --- 5) create binary aurora label ---\n",
        "# threshold selection: default 20 (from project); you can adjust or compute percentile\n",
        "AE_THRESHOLD = 20\n",
        "matched['aurora_label'] = (matched['AE'] > AE_THRESHOLD).astype(int)\n",
        "print(\"Aurora label distribution:\")\n",
        "print(matched['aurora_label'].value_counts())\n",
        "\n",
        "# --- 6) save full dataset & train/test split ---\n",
        "OUT_FULL = os.path.join(BASE_DIR, 'Aurora_Classification_dataset.csv')\n",
        "OUT_TRAIN = os.path.join(BASE_DIR, 'Aurora_Train.csv')\n",
        "OUT_TEST  = os.path.join(BASE_DIR, 'Aurora_Test.csv')\n",
        "\n",
        "matched.to_csv(OUT_FULL, index=False)\n",
        "print(\"Saved full classification dataset to:\", OUT_FULL)\n",
        "\n",
        "# Stratified split\n",
        "feature_cols = [c for c in matched.columns if c not in ['time','datetime','aurora_label','AE','source_file']]\n",
        "X = matched[feature_cols]\n",
        "y = matched['aurora_label']\n",
        "\n",
        "# If not enough positive samples, adjust stratify parameter\n",
        "stratify = y if y.nunique() > 1 and y.sum() > 5 else None\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=stratify)\n",
        "\n",
        "train_data = matched.loc[X_train.index]\n",
        "test_data  = matched.loc[X_test.index]\n",
        "\n",
        "train_data.to_csv(OUT_TRAIN, index=False)\n",
        "test_data.to_csv(OUT_TEST, index=False)\n",
        "print(\"Saved train/test CSVs:\")\n",
        "print(\" -\", OUT_TRAIN, \"(\", len(train_data), \"rows )\")\n",
        "print(\" -\", OUT_TEST, \"(\", len(test_data), \"rows )\")\n",
        "\n",
        "# quick class balance check\n",
        "print(\"\\nTrain class balance:\", train_data['aurora_label'].value_counts(normalize=True).to_dict())\n",
        "print(\"Test class balance:\", test_data['aurora_label'].value_counts(normalize=True).to_dict())\n",
        "\n",
        "# done\n",
        "print(\"\\nAE merge & labeling complete. If class imbalance is extreme, consider adjusting AE_THRESHOLD or using different labeling strategy.\")"
      ],
      "metadata": {
        "id": "3x-8tZ0Mfx2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification training (model with best fit)"
      ],
      "metadata": {
        "id": "rPNPW_6CgCXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Classification training - use regression meta-features + train classifiers\n",
        "import os, joblib, pandas as pd, numpy as np, traceback\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, average_precision_score, f1_score, accuracy_score, matthews_corrcoef\n",
        "\n",
        "print(\"Loading train/test CSVs...\")\n",
        "TRAIN_PATH = os.path.join(BASE_DIR, 'Aurora_Train.csv')\n",
        "TEST_PATH  = os.path.join(BASE_DIR, 'Aurora_Test.csv')\n",
        "if not os.path.exists(TRAIN_PATH) or not os.path.exists(TEST_PATH):\n",
        "    raise FileNotFoundError(\"Train/Test CSVs not found. Run AE merge cell (Cell 10).\")\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
        "test_df  = pd.read_csv(TEST_PATH, low_memory=False)\n",
        "print(\"Train rows:\", len(train_df), \"Test rows:\", len(test_df))\n",
        "\n",
        "# Features to exclude\n",
        "exclude_cols = ['datetime','aurora_label','source_file','time','AE']\n",
        "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
        "print(\"Number of base features:\", len(feature_cols))\n",
        "\n",
        "X_train_raw = train_df[feature_cols].fillna(0)\n",
        "X_test_raw  = test_df[feature_cols].fillna(0)\n",
        "y_train = train_df['aurora_label']\n",
        "y_test  = test_df['aurora_label']\n",
        "\n",
        "# ---------------- Create meta-features from regression models ----------------\n",
        "print(\"\\nAttempting to load regression models for meta-features...\")\n",
        "reg_models_dir = os.path.join(MODELS_DIR, 'regression')\n",
        "rf_model_path = os.path.join(reg_models_dir, 'random_forest_model.joblib')\n",
        "xgb_model_path = os.path.join(reg_models_dir, 'xgboost_model.joblib')\n",
        "lstm_model_path = os.path.join(reg_models_dir, 'lstm_model.keras')\n",
        "\n",
        "models = {'rf': None, 'xgb': None, 'lstm': None}\n",
        "try:\n",
        "    if os.path.exists(rf_model_path):\n",
        "        models['rf'] = joblib.load(rf_model_path)\n",
        "        print(\"Loaded RF regressor.\")\n",
        "    if os.path.exists(xgb_model_path):\n",
        "        models['xgb'] = joblib.load(xgb_model_path)\n",
        "        print(\"Loaded XGB regressor.\")\n",
        "    if os.path.exists(lstm_model_path):\n",
        "        try:\n",
        "            from tensorflow import keras\n",
        "            models['lstm'] = keras.models.load_model(lstm_model_path)\n",
        "            print(\"Loaded LSTM regressor.\")\n",
        "        except Exception as e:\n",
        "            print(\"LSTM load failed:\", e)\n",
        "except Exception as e:\n",
        "    print(\"Regression model load error:\", e)\n",
        "\n",
        "# Align sequences for LSTM meta-feature if possible\n",
        "SEQ_LEN = 10\n",
        "def create_sequences(X, seq_len):\n",
        "    X_seq = []\n",
        "    for i in range(len(X) - seq_len + 1):\n",
        "        X_seq.append(X[i:i+seq_len])\n",
        "    return np.array(X_seq)\n",
        "\n",
        "X_train_aligned = X_train_raw.copy().reset_index(drop=True)\n",
        "X_test_aligned  = X_test_raw.copy().reset_index(drop=True)\n",
        "\n",
        "# Add RF/XGB predictions as meta-features if models available\n",
        "if models['rf'] is not None:\n",
        "    try:\n",
        "        X_train_aligned['rf_pred'] = models['rf'].predict(X_train_raw.values)\n",
        "        X_test_aligned['rf_pred']  = models['rf'].predict(X_test_raw.values)\n",
        "        print(\"Added rf_pred meta-feature.\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to generate rf_pred:\", e)\n",
        "\n",
        "if models['xgb'] is not None:\n",
        "    try:\n",
        "        X_train_aligned['xgb_pred'] = models['xgb'].predict(X_train_raw.values)\n",
        "        X_test_aligned['xgb_pred']  = models['xgb'].predict(X_test_raw.values)\n",
        "        print(\"Added xgb_pred meta-feature.\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to generate xgb_pred:\", e)\n",
        "\n",
        "# LSTM meta-feature: ensure sequence lengths align\n",
        "if models['lstm'] is not None:\n",
        "    try:\n",
        "        X_train_seq = create_sequences(X_train_raw.values, SEQ_LEN)\n",
        "        X_test_seq  = create_sequences(X_test_raw.values, SEQ_LEN)\n",
        "        # predictions length will be len(X_seq); align by trimming aligned frames\n",
        "        train_preds = models['lstm'].predict(X_train_seq, verbose=0).flatten()\n",
        "        test_preds  = models['lstm'].predict(X_test_seq, verbose=0).flatten()\n",
        "        # align by trimming first SEQ_LEN-1 rows\n",
        "        X_train_aligned = X_train_aligned.iloc[SEQ_LEN-1:].reset_index(drop=True)\n",
        "        X_test_aligned  = X_test_aligned.iloc[SEQ_LEN-1:].reset_index(drop=True)\n",
        "        y_train_aligned = y_train.iloc[SEQ_LEN-1:].reset_index(drop=True)\n",
        "        y_test_aligned  = y_test.iloc[SEQ_LEN-1:].reset_index(drop=True)\n",
        "        X_train_aligned['lstm_pred'] = train_preds\n",
        "        X_test_aligned['lstm_pred'] = test_preds\n",
        "        print(\"Added lstm_pred meta-feature and aligned sequences.\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to add lstm_pred:\", e)\n",
        "        # fallback to non-sequence alignment\n",
        "        y_train_aligned = y_train.copy()\n",
        "        y_test_aligned = y_test.copy()\n",
        "else:\n",
        "    y_train_aligned = y_train.copy()\n",
        "    y_test_aligned = y_test.copy()\n",
        "\n",
        "# If LSTM not used, ensure indexes align\n",
        "if 'lstm_pred' not in X_train_aligned.columns:\n",
        "    # make sure y_train_aligned/y_test_aligned set\n",
        "    y_train_aligned = y_train.copy()\n",
        "    y_test_aligned = y_test.copy()\n",
        "\n",
        "print(\"Final train samples:\", len(X_train_aligned), \"Final test samples:\", len(X_test_aligned))\n",
        "\n",
        "# ---------------- Train classifiers ----------------\n",
        "print(\"\\nTraining classifiers...\")\n",
        "\n",
        "# Standardize for logistic regression\n",
        "scaler = StandardScaler()\n",
        "Xtr_scaled = scaler.fit_transform(X_train_aligned)\n",
        "Xte_scaled = scaler.transform(X_test_aligned)\n",
        "\n",
        "out_dir = os.path.join(MODELS_DIR, 'classification')\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=200 if not FAST_MODE else 100, max_depth=25, class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "rf_clf.fit(X_train_aligned, y_train_aligned)\n",
        "y_pred_rf = rf_clf.predict(X_test_aligned)\n",
        "y_proba_rf = rf_clf.predict_proba(X_test_aligned)[:,1]\n",
        "results['RandomForest'] = {\n",
        "    'accuracy': accuracy_score(y_test_aligned, y_pred_rf),\n",
        "    'f1_score': f1_score(y_test_aligned, y_pred_rf),\n",
        "    'roc_auc': roc_auc_score(y_test_aligned, y_proba_rf),\n",
        "    'avg_precision': average_precision_score(y_test_aligned, y_proba_rf),\n",
        "    'mcc': matthews_corrcoef(y_test_aligned, y_pred_rf),\n",
        "    'y_pred': y_pred_rf,\n",
        "    'y_proba': y_proba_rf\n",
        "}\n",
        "joblib.dump(rf_clf, os.path.join(out_dir, 'random_forest.joblib'))\n",
        "print(\"Trained & saved RandomForest classifier.\")\n",
        "\n",
        "# XGBoost classifier\n",
        "import xgboost as xgb\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=200 if not FAST_MODE else 100, max_depth=8, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', n_jobs=-1, random_state=42)\n",
        "xgb_clf.fit(X_train_aligned, y_train_aligned)\n",
        "y_pred_xgb = xgb_clf.predict(X_test_aligned)\n",
        "y_proba_xgb = xgb_clf.predict_proba(X_test_aligned)[:,1]\n",
        "results['XGBoost'] = {\n",
        "    'accuracy': accuracy_score(y_test_aligned, y_pred_xgb),\n",
        "    'f1_score': f1_score(y_test_aligned, y_pred_xgb),\n",
        "    'roc_auc': roc_auc_score(y_test_aligned, y_proba_xgb),\n",
        "    'avg_precision': average_precision_score(y_test_aligned, y_proba_xgb),\n",
        "    'mcc': matthews_corrcoef(y_test_aligned, y_pred_xgb),\n",
        "    'y_pred': y_pred_xgb,\n",
        "    'y_proba': y_proba_xgb\n",
        "}\n",
        "joblib.dump(xgb_clf, os.path.join(out_dir, 'xgboost.joblib'))\n",
        "print(\"Trained & saved XGBoost classifier.\")\n",
        "\n",
        "# LightGBM (optional)\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    lgb_clf = lgb.LGBMClassifier(n_estimators=200 if not FAST_MODE else 100, max_depth=15, class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "    lgb_clf.fit(X_train_aligned, y_train_aligned)\n",
        "    y_pred_lgb = lgb_clf.predict(X_test_aligned)\n",
        "    y_proba_lgb = lgb_clf.predict_proba(X_test_aligned)[:,1]\n",
        "    results['LightGBM'] = {\n",
        "        'accuracy': accuracy_score(y_test_aligned, y_pred_lgb),\n",
        "        'f1_score': f1_score(y_test_aligned, y_pred_lgb),\n",
        "        'roc_auc': roc_auc_score(y_test_aligned, y_proba_lgb),\n",
        "        'avg_precision': average_precision_score(y_test_aligned, y_proba_lgb),\n",
        "        'mcc': matthews_corrcoef(y_test_aligned, y_pred_lgb),\n",
        "        'y_pred': y_pred_lgb,\n",
        "        'y_proba': y_proba_lgb\n",
        "    }\n",
        "    joblib.dump(lgb_clf, os.path.join(out_dir, 'lightgbm.joblib'))\n",
        "    print(\"Trained & saved LightGBM classifier.\")\n",
        "except Exception as e:\n",
        "    print(\"LightGBM not available or failed:\", e)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "lr.fit(Xtr_scaled, y_train_aligned)\n",
        "y_pred_lr = lr.predict(Xte_scaled)\n",
        "# get probabilities using scaler applied test set\n",
        "y_proba_lr = lr.predict_proba(Xte_scaled)[:,1]\n",
        "results['LogisticRegression'] = {\n",
        "    'accuracy': accuracy_score(y_test_aligned, y_pred_lr),\n",
        "    'f1_score': f1_score(y_test_aligned, y_pred_lr),\n",
        "    'roc_auc': roc_auc_score(y_test_aligned, y_proba_lr),\n",
        "    'avg_precision': average_precision_score(y_test_aligned, y_proba_lr),\n",
        "    'mcc': matthews_corrcoef(y_test_aligned, y_pred_lr),\n",
        "    'y_pred': y_pred_lr,\n",
        "    'y_proba': y_proba_lr\n",
        "}\n",
        "joblib.dump(lr, os.path.join(out_dir, 'logistic_regression_model.joblib'))\n",
        "joblib.dump(scaler, os.path.join(out_dir, 'logistic_regression_scaler.joblib'))\n",
        "print(\"Trained & saved Logistic Regression.\")\n",
        "\n",
        "# ---------------- Save results summary ----------------\n",
        "summary = pd.DataFrame({name: {k:v for k,v in vals.items() if k not in ['y_pred','y_proba']} for name,vals in results.items()}).T\n",
        "summary.to_csv(os.path.join(out_dir, 'model_comparison_results.csv'))\n",
        "print(\"\\nClassification results summary:\")\n",
        "print(summary)\n",
        "\n",
        "# Save best model info\n",
        "best_model = summary['roc_auc'].idxmax()\n",
        "with open(os.path.join(out_dir, 'best_model_info.txt'),'w') as f:\n",
        "    f.write(f\"Best Model: {best_model}\\nROC-AUC: {summary.loc[best_model,'roc_auc']:.4f}\\n\")\n",
        "print(\"Saved models to:\", out_dir)"
      ],
      "metadata": {
        "id": "yJFHeeBzgGMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA ANALYTICS AND FINAL PLOTTING"
      ],
      "metadata": {
        "id": "T4tUq9TbggI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Data analytics & plotting - dataset overview, distributions, correlations, PCA/t-SNE, pairplots\n",
        "import os, pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy import stats\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "OUTPUT_DIR = os.path.join(PLOTS_DIR, 'data_analytics')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_PATH = os.path.join(BASE_DIR, 'Aurora_Train.csv')\n",
        "TEST_PATH  = os.path.join(BASE_DIR, 'Aurora_Test.csv')\n",
        "\n",
        "print(\"Loading:\", TRAIN_PATH, TEST_PATH)\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df  = pd.read_csv(TEST_PATH)\n",
        "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
        "\n",
        "# Identify feature columns\n",
        "exclude_cols = ['aurora_label','datetime','source_file','time','AE']\n",
        "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
        "FEATURES_TO_PLOT = feature_cols[:8] if len(feature_cols) > 8 else feature_cols\n",
        "\n",
        "# 1) Dataset overview: class distribution & missing\n",
        "fig, axes = plt.subplots(2,2, figsize=(14,8))\n",
        "ax = axes[0,0]\n",
        "counts = train_df['aurora_label'].value_counts()\n",
        "ax.bar(['Quiet (0)','Active (1)'], counts.values, color=['#2ecc71','#e74c3c'], alpha=0.8, edgecolor='black')\n",
        "ax.set_title('Aurora Activity Distribution (Train)')\n",
        "for i,v in enumerate(counts.values):\n",
        "    ax.text(i, v + max(counts.values)*0.01, f'{v}\\n({v/len(train_df)*100:.1f}%)', ha='center')\n",
        "\n",
        "ax = axes[0,1]\n",
        "missing = train_df[feature_cols].isnull().sum().sort_values(ascending=False).head(15)\n",
        "if missing.sum() > 0:\n",
        "    ax.barh(range(len(missing)), missing.values, color='coral', edgecolor='black')\n",
        "    ax.set_yticks(range(len(missing))); ax.set_yticklabels(missing.index)\n",
        "    ax.set_title('Missing Values by Feature')\n",
        "else:\n",
        "    ax.text(0.5,0.5,'No Missing Values ✓', ha='center', va='center')\n",
        "    ax.axis('off')\n",
        "\n",
        "ax = axes[1,0]\n",
        "corrs = []\n",
        "for col in feature_cols[:20]:\n",
        "    if train_df[col].dtype in ['float64','int64']:\n",
        "        corr = train_df[col].corr(train_df['aurora_label'])\n",
        "        corrs.append((col, corr))\n",
        "if corrs:\n",
        "    corrs = sorted(corrs, key=lambda x: abs(x[1]), reverse=True)[:10]\n",
        "    cols, vals = zip(*corrs)\n",
        "    colors_corr = ['#e74c3c' if v>0 else '#3498db' for v in vals]\n",
        "    ax.barh(range(len(cols)), vals, color=colors_corr, edgecolor='black')\n",
        "    ax.set_yticks(range(len(cols))); ax.set_yticklabels(cols)\n",
        "    ax.set_title('Top 10 Feature Correlations (abs)')\n",
        "ax = axes[1,1]; ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '00_dataset_overview.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved 00_dataset_overview.png\")\n",
        "\n",
        "# 2) Feature distributions (KDE)\n",
        "n_features = len(FEATURES_TO_PLOT)\n",
        "n_cols = 3\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
        "axes = axes.flatten()\n",
        "for idx, feat in enumerate(FEATURES_TO_PLOT):\n",
        "    ax = axes[idx]\n",
        "    sns.kdeplot(train_df[feat].dropna(), label='Train', ax=ax, fill=True, alpha=0.5)\n",
        "    sns.kdeplot(test_df[feat].dropna(), label='Test', ax=ax, fill=True, alpha=0.5)\n",
        "    ax.set_title(feat)\n",
        "    ax.legend(fontsize=7)\n",
        "for idx in range(n_features, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '01_feature_distributions.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved 01_feature_distributions.png\")\n",
        "\n",
        "# 3) Class-wise comparisons (violin + t-test)\n",
        "fig, axes = plt.subplots((len(FEATURES_TO_PLOT)+1)//2, 2, figsize=(14, 4*((len(FEATURES_TO_PLOT)+1)//2)))\n",
        "axes = axes.flatten()\n",
        "for idx, feat in enumerate(FEATURES_TO_PLOT):\n",
        "    ax = axes[idx]\n",
        "    sns.violinplot(x='aurora_label', y=feat, data=train_df, ax=ax, palette={0:'#2ecc71',1:'#e74c3c'}, cut=0)\n",
        "    quiet_vals = train_df[train_df['aurora_label']==0][feat].dropna()\n",
        "    active_vals = train_df[train_df['aurora_label']==1][feat].dropna()\n",
        "    if len(quiet_vals)>0 and len(active_vals)>0:\n",
        "        tstat, pval = stats.ttest_ind(quiet_vals, active_vals, equal_var=False)\n",
        "        sig = '***' if pval<0.001 else '**' if pval<0.01 else '*' if pval<0.05 else 'ns'\n",
        "        ax.text(0.5, 0.95, f'p={pval:.2e} {sig}', transform=ax.transAxes, ha='center', fontsize=9, bbox=dict(facecolor='yellow', alpha=0.3))\n",
        "for idx in range(len(FEATURES_TO_PLOT), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '02_classwise_comparison.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved 02_classwise_comparison.png\")\n",
        "\n",
        "# 4) Time-series analysis for top 4 features\n",
        "for feature in FEATURES_TO_PLOT[:4]:\n",
        "    fig, axes = plt.subplots(2,1, figsize=(14,8), gridspec_kw={'height_ratios':[3,1]})\n",
        "    if 'datetime' in train_df.columns:\n",
        "        x_axis = pd.to_datetime(train_df['datetime'])\n",
        "    else:\n",
        "        x_axis = train_df.index\n",
        "    window = min(100, max(10, len(train_df)//10))\n",
        "    rolling_mean = train_df[feature].rolling(window=window, center=True).mean()\n",
        "    rolling_std = train_df[feature].rolling(window=window, center=True).std()\n",
        "    axes[0].plot(x_axis, train_df[feature], alpha=0.3, linewidth=0.5, label='Raw')\n",
        "    axes[0].plot(x_axis, rolling_mean, color='blue', linewidth=2, label=f'Rolling mean (w={window})')\n",
        "    axes[0].fill_between(x_axis, rolling_mean-rolling_std, rolling_mean+rolling_std, alpha=0.2, color='blue')\n",
        "    active_mask = train_df['aurora_label']==1\n",
        "    axes[0].scatter(x_axis[active_mask], train_df[feature][active_mask], color='red', s=8, alpha=0.6, label='Active')\n",
        "    axes[0].legend()\n",
        "    axes[1].fill_between(x_axis, 0, train_df['aurora_label'], color='red', alpha=0.5, step='mid')\n",
        "    axes[1].set_ylim(-0.1,1.1)\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(OUTPUT_DIR, f'03_timeseries_{feature}.png')\n",
        "    plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"Saved\", fname)\n",
        "\n",
        "# 5) Correlation matrix (top 20 features)\n",
        "subset = feature_cols[:20]\n",
        "if len(subset) > 1:\n",
        "    corr = train_df[subset].corr()\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.heatmap(corr, cmap='coolwarm', center=0)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, '04_correlation_matrix.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"Saved 04_correlation_matrix.png\")\n",
        "\n",
        "# 6) PCA and t-SNE visualizations\n",
        "X = train_df[feature_cols].fillna(0).values\n",
        "y = train_df['aurora_label'].values\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='coolwarm', s=8, alpha=0.6)\n",
        "plt.title(f'PCA projection (PC1 {pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sample_n = 2000 if not FAST_MODE else 1000\n",
        "from sklearn.manifold import TSNE\n",
        "X_sub = X[:sample_n]\n",
        "y_sub = y[:sample_n]\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X_sub)\n",
        "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_sub, cmap='coolwarm', s=8, alpha=0.6)\n",
        "plt.title('t-SNE projection')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '06_dimensionality_reduction.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved 06_dimensionality_reduction.png\")\n",
        "\n",
        "# 7) Pairplot of top features (sampled)\n",
        "top4 = FEATURES_TO_PLOT[:4]\n",
        "pair_df = train_df[top4 + ['aurora_label']].sample(n=min(2000, len(train_df)), random_state=42)\n",
        "g = sns.pairplot(pair_df, hue='aurora_label', diag_kind='kde', plot_kws={'alpha':0.6, 's':20})\n",
        "g.fig.suptitle('Pairplot (top 4 features)', y=1.02)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '07_pairplot.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved 07_pairplot.png\")\n",
        "\n",
        "print(\"All analytics plots saved to:\", OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "iEoq5KdYgkMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL SUMMARY AND CONCLUSION DISPLAY"
      ],
      "metadata": {
        "id": "2nPDckPig3Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Summary, quick evaluation snippets, and package artifacts for download\n",
        "import os, pandas as pd, joblib, glob, shutil, json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Base directories:\")\n",
        "print(\" BASE_DIR:\", BASE_DIR)\n",
        "print(\" MODELS_DIR:\", MODELS_DIR)\n",
        "print(\" PLOTS_DIR:\", PLOTS_DIR)\n",
        "\n",
        "# 1) List produced files\n",
        "print(\"\\nProduced files (sample):\")\n",
        "for pattern in [\n",
        "    os.path.join(BASE_DIR, \"*.csv\"),\n",
        "    os.path.join(MODELS_DIR, \"**\", \"*.*\"),\n",
        "    os.path.join(PLOTS_DIR, \"**\", \"*.*\")\n",
        "]:\n",
        "    matches = glob.glob(pattern, recursive=True)\n",
        "    print(f\" {pattern} -> {len(matches)} files (showing up to 5):\")\n",
        "    for m in matches[:5]:\n",
        "        print(\"   -\", m)\n",
        "print()\n",
        "\n",
        "# 2) Show model comparison files if present\n",
        "reg_comp = os.path.join(MODELS_DIR, 'regression', 'model_comparison.csv')\n",
        "clf_comp = os.path.join(MODELS_DIR, 'classification', 'model_comparison_results.csv')\n",
        "if os.path.exists(reg_comp):\n",
        "    print(\"Regression comparison:\")\n",
        "    display(pd.read_csv(reg_comp).head())\n",
        "if os.path.exists(clf_comp):\n",
        "    print(\"Classification comparison:\")\n",
        "    display(pd.read_csv(clf_comp).head())\n",
        "\n",
        "# 3) Quick: print best classification model info if available\n",
        "best_info = os.path.join(MODELS_DIR, 'classification', 'best_model_info.txt')\n",
        "if os.path.exists(best_info):\n",
        "    print(\"\\nBest classification model:\")\n",
        "    print(open(best_info).read())\n",
        "else:\n",
        "    print(\"\\nNo best_model_info.txt found in classification folder.\")\n",
        "\n",
        "# 4) Quick inference demo (uses best saved classifier)\n",
        "demo_out = os.path.join(BASE_DIR, 'inference_demo.csv')\n",
        "try:\n",
        "    # load test data\n",
        "    test_path = os.path.join(BASE_DIR, 'Aurora_Test.csv')\n",
        "    if os.path.exists(test_path):\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        feat_cols = [c for c in test_df.columns if c not in ['datetime','aurora_label','time','AE','source_file']]\n",
        "        sample_X = test_df[feat_cols].fillna(0).iloc[:50]\n",
        "        # try to load best classifier by name from best_model_info or fallback to random_forest\n",
        "        model_paths = {\n",
        "            'RandomForest': os.path.join(MODELS_DIR, 'classification', 'random_forest.joblib'),\n",
        "            'XGBoost': os.path.join(MODELS_DIR, 'classification', 'xgboost.joblib'),\n",
        "            'LightGBM': os.path.join(MODELS_DIR, 'classification', 'lightgbm.joblib'),\n",
        "            'LogisticRegression': os.path.join(MODELS_DIR, 'classification', 'logistic_regression_model.joblib')\n",
        "        }\n",
        "        chosen = None\n",
        "        if os.path.exists(best_info):\n",
        "            txt = open(best_info).read().lower()\n",
        "            for k in model_paths:\n",
        "                if k.lower() in txt:\n",
        "                    chosen = k\n",
        "                    break\n",
        "        if chosen is None:\n",
        "            chosen = 'RandomForest' if os.path.exists(model_paths['RandomForest']) else next((k for k,v in model_paths.items() if os.path.exists(v)), None)\n",
        "        if chosen is not None and os.path.exists(model_paths[chosen]):\n",
        "            print(f\"\\nRunning demo inference with: {chosen}\")\n",
        "            clf = joblib.load(model_paths[chosen])\n",
        "            # if logistic regression, scale if scaler exists\n",
        "            if chosen == 'LogisticRegression':\n",
        "                scaler_path = os.path.join(MODELS_DIR, 'classification', 'logistic_regression_scaler.joblib')\n",
        "                if os.path.exists(scaler_path):\n",
        "                    scaler = joblib.load(scaler_path)\n",
        "                    sample_X_s = scaler.transform(sample_X)\n",
        "                else:\n",
        "                    sample_X_s = sample_X.values\n",
        "                preds = clf.predict_proba(sample_X_s)[:,1] if hasattr(clf, 'predict_proba') else clf.predict(sample_X_s)\n",
        "            else:\n",
        "                preds = clf.predict_proba(sample_X.values)[:,1] if hasattr(clf, 'predict_proba') else clf.predict(sample_X.values)\n",
        "            demo_df = sample_X.reset_index(drop=True).iloc[:, :10].copy()  # show first 10 cols for brevity\n",
        "            demo_df['aurora_prob'] = preds\n",
        "            demo_df.to_csv(demo_out, index=False)\n",
        "            print(\"Saved inference demo to:\", demo_out)\n",
        "            display(demo_df.head())\n",
        "        else:\n",
        "            print(\"No classifier found for demo inference.\")\n",
        "    else:\n",
        "        print(\"Test CSV missing; cannot run demo inference.\")\n",
        "except Exception as e:\n",
        "    print(\"Demo inference failed:\", e)\n",
        "\n",
        "# 5) Zip artifacts for easy download\n",
        "zip_name = os.path.join(BASE_DIR, 'aurora_pipeline_artifacts.zip')\n",
        "print(\"\\nCreating zip of models & plots (this may take a moment)...\")\n",
        "with shutil.ZipFile(zip_name, 'w') as zf:\n",
        "    # add models\n",
        "    for f in glob.glob(os.path.join(MODELS_DIR, '**', '*.*'), recursive=True):\n",
        "        zf.write(f, arcname=os.path.join('models', os.path.relpath(f, MODELS_DIR)))\n",
        "    # add plots\n",
        "    for f in glob.glob(os.path.join(PLOTS_DIR, '**', '*.*'), recursive=True):\n",
        "        zf.write(f, arcname=os.path.join('plots', os.path.relpath(f, PLOTS_DIR)))\n",
        "    # add CSVs\n",
        "    for f in glob.glob(os.path.join(BASE_DIR, '*.csv')):\n",
        "        zf.write(f, arcname=os.path.join('data', os.path.basename(f)))\n",
        "print(\"Created zip:\", zip_name)\n",
        "print(\"You can download it from Colab using the Files sidebar or:\")\n",
        "print(f\"  from google.colab import files\\n  files.download('{zip_name}')\")\n",
        "\n",
        "# 6) Next steps summary\n",
        "notes = {\n",
        "    \"next_steps\": [\n",
        "        \"Inspect model metrics and confusion matrices under models/classification.\",\n",
        "        \"Tune AE threshold or use percentile-based labeling if class imbalance is extreme.\",\n",
        "        \"If performance is acceptable, export the best model to a serving format (ONNX / TF SavedModel) and build an inference wrapper.\",\n",
        "        \"Set FAST_MODE=False and re-run training cells for full training.\",\n",
        "        \"Consider logging experiments and hyperparams (MLflow or wandb).\"\n",
        "    ],\n",
        "    \"artifact_locations\": {\n",
        "        \"models_dir\": MODELS_DIR,\n",
        "        \"plots_dir\": PLOTS_DIR,\n",
        "        \"data_dir\": BASE_DIR,\n",
        "        \"zip\": zip_name\n",
        "    }\n",
        "}\n",
        "print(\"\\nNext steps (brief):\")\n",
        "for i, s in enumerate(notes['next_steps'], 1):\n",
        "    print(f\" {i}. {s}\")\n",
        "print(\"\\nArtifact locations:\")\n",
        "print(json.dumps(notes['artifact_locations'], indent=2))"
      ],
      "metadata": {
        "id": "y1EdPbMqg6UX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}